---
edit_url: "https://github.com/dhis2/dhis2-docs/blob/2.41/src/developer/web-api/data-validation.md"
revision_date: '2024-03-13'
tags:
- Develop
- DHIS core version 2.41
---

# 数据验证 { #data-validation } 

## 验证方式 { #webapi_validation } 

要生成数据验证摘要，您可以与
验证资源。数据集资源针对数据输入进行了优化
用于验证数据集/表单的客户端，可以像这样访问：

    GET /api/33/validation/dataSet/QX4ZTUbOt3a.json?pe=201501&ou=DiszpKrYNg8

除了基于数据集验证规则外，还有两种
执行验证的其他方法：自定义验证和
预定验证。

第一个路径变量是引用数据集的标识符
证实。支持 XML 和 JSON 资源表示。这
响应包含违反验证规则。这将延长
在即将到来的版本中有更多的验证类型。

要检索与特定数据集相关的验证规则，
意思是所有数据元素都是一部分的带有公式的验证规则
的特定数据集，您可以向
`validationRules` 资源如下：

    GET /api/validationRules?dataSet=<dataset-id>

验证规则有左边和右边，也就是
根据运营商比较有效性。有效的运算符
值见下表。



Table: Operators

| 值 | 描述 |
|---|---|
| equal_to | Equal to |
| not_equal_to | Not equal to |
| greater_than | Greater than |
| greater_than_or_equal_to | Greater than or equal to |
| less_than | Less than |
| less_than_or_equal_to | Less than or equal to |
| compulsory_pair | If either side is present, the other must also be |
| exclusive_pair | If either side is present, the other must not be |

左边和右边的表达式是数学表达式
其中可以包含对数据元素和类别选项的引用
以下格式的组合：

    $ {<dataelement-id>。 <catoptcombo-id>}

左侧和右侧表达式有一个 *missing 值
战略*。这是指系统应该如何处理数据值
缺少数据元素/类别选项组合引用
在公式中是否应该检查验证规则
为有效性或跳过。有效的缺失值策略见于
下表。



Table: Missing value strategies

| 值 | 描述 |
|---|---|
| SKIP_IF_ANY_VALUE_MISSING | Skip validation rule if any data value is missing |
| SKIP_IF_ALL_VALUES_MISSING | Skip validation rule if all data values are missing |
| NEVER_SKIP | Never skip validation rule irrespective of missing data values |

## Validation results { #webapi_validation_results } 

验证结果是在执行期间发现的违规的持久结果
验证分析。如果您在开始时选择“持久结果”或
安排验证分析，发现的任何违规将存储在
数据库。当结果存储在数据库中时，它将被使用
对于 3 件事：

1.  根据存储的结果生成分析。

2.  未生成通知的持久结果将这样做，
    一次。

3.  跟踪结果是否产生了
    通知。

4.  跳过运行时已经检查过的规则
    验证分析。

这意味着如果你不坚持你的结果，你将无法
为验证结果生成分析，如果选中，结果将
每次找到并运行验证时生成通知
分析可能会更慢。

### Query validation results { #query-validation-results } 

持久化的验证结果可以在下面查看
端点：

    GET /api/33/validationResults

您还可以使用验证结果 ID 检查单个结果
在这个端点：

    GET /api/33/validationResults/<id>

验证结果也可以通过以下属性过滤：

* 组织单位：`ou = <UID>`
* 验证规则：`vr = <UID>`
* 期间：`pe = <ISO-expression>`

上面的每个过滤器属性可以多次出现，例如：

    GET /api/36/validationResults?ou=jNb63DIHuwU&ou=RzgSFJ9E46G

同一过滤器的多个值与OR组合，结果必须匹配给定值之一。

如果使用了一个以上的过滤器属性，则将它们与AND组合在一起，结果必须与每个属性的值之一匹配。

对于时段过滤器，匹配结果必须与任何指定的时段重叠。

此外，验证结果还可以按其创建日期进行过滤：

    GET /api/36/validationResults?createdDate=<date>

该过滤器可以与其他任何过滤器结合使用。

### Trigger validation result notifications { #trigger-validation-result-notifications } 

Validation results are sent out to the appropriate users once every day,
but can also be manually triggered to run on demand using the following
API endpoint:

    POST / api / 33 / validation / sendNotifications

使用此端点仅发送未发送的结果。

### Delete validation results { #delete-validation-results } 

验证结果可以通过ID手动删除，

    删除/ api / 36 / validationResults / <id>

或使用过滤器

    删除/ api / 36 / validationResults？ <filters>

Supported filter parameters include:

* `ou = <UID>`以匹配组织单位的所有验证结果；提供多个参数时，多个单元组合或
* `vr = <UID>`以匹配验证规则的所有验证结果；提供多个参数时，多个规则组合或
* `pe = <ISO-expression>`以匹配与与指定时期重叠的时期相关的所有验证结果
* `created = <ISO-expression>`以匹配在规定时间内创建的所有验证结果
* `notificationSent=<boolean>` to match either only validation results for which a notification was or wasn't sent

如果组合了过滤器，则所有条件都必须为真（AND逻辑）。

Some examples:

要删除 2020 年第一季度与 UID 为`NqwvaQC1ni4`的组织单位相关的所有验证结果，请使用：

```
DELETE /api/36/validationResults?ou=NqwvaQC1ni4&pe=2020Q1
```

要删除在2019年第1周创建的且已发送通知的所有验证结果，请使用：

```
DELETE /api/36/validationResults?created=2019W1&notificationSent=true
```

Any delete operation will require the authority _Perform maintenance tasks_.


## 离群值检测 { #outlier-detection } 

The outlier detection endpoint allows for detecting outliers in aggregate data values.

```
GET / api / 36 / outlierDetection
```

该端点支持两种用于检测离群值的算法：

* ** Z分数：** Z分数定义为分数与平均值之间的绝对偏差除以标准偏差。必须使用z分数算法指定一个阈值参数，该阈值参数表示与平均值之间的标准偏差，以定义异常值的上限和下限。
* **Modified Z-score:** Same as z-score except it uses the median instead of the mean as measure of central tendency. Parameters are same as for Z-score.
* ** Min-max：** Min-max数据元素值是指可以根据数据元素，组织单位和类别选项组合插入DHIS 2的自定义边界。

离群值将*根据显着性*排序，默认情况下是与均值的绝对偏差，最高有效值在前。这有助于快速识别对数据质量和数据分析影响最大的离群值。

### 请求查询参数 { #request-query-parameters } 

支持以下查询参数。

| 查询参数 | 描述                                                  | 强制的 | 选项（默认为默认）                   |
| --------------- | ------------------------------------------------------------ | --------- | ----------------------------------------- |
| ds              | 数据集，可以多次指定。                   | 不 [*]    | 数据集标识符。                      |
| 德              | 数据元素，可以多次指定。               | 不 [*]    | 数据元素标识符。                  |
| 开始日期       | 间隔的开始日期，以检查异常值。               | 是的       | 日期（yyyy-MM-dd）。                        |
| 结束日期         | 检查异常值的时间间隔的结束日期。                 | 是的       | 日期（yyyy-MM-dd）。                        |
| 欧              | 组织单位，可以多次指定。          | 是的       | 组织单位标识符。             |
| 算法       | 用于离群值检测的算法。                      | 不        | `Z_SCORE`, `MIN_MAX`, `MOD_Z_SCORE`       |
| 临界点       | Threshold for outlier values. `Z_SCORE` and `MOD_Z_SCORE` algorithm only. | 不        | 数值，大于零。默认值：3.0。 |
| dataStartDate   | Start date for interval for mean and std dev calculation. `Z_SCORE` and `MOD_Z_SCORE` algorithm only. | 不        | 日期（yyyy-MM-dd）。 |
| dataEndDate     | End date for interval for mean and std dev calculation. `Z_SCORE` and `MOD_Z_SCORE` algorithm only. | 不        | 日期（yyyy-MM-dd）。   |
| 订购         | Field to order by. `Z_SCORE` and `MOD_Z_SCORE`algorithm only.| 不        | `MEAN_ABS_DEV`，`Z_SCORE`                 |
| maxResults      | 输出的最大限制。                                    | 不        | 整数，大于零。默认值：500。 |

[*]  You must specify either data sets with the `ds` parameter, which will include all data elements in the data sets, _or_ specify data elements with the `de` parameter.

必须定义至少一个数据集或数据元素，开始日期和结束日期以及至少一个组织单位。

The `startDate` and `endDate` parameters are mandatory and refer to the time interval for which you want to detect outliers. The `dataStartDate` and `dataEndDate` parameters are optional and refer to the time interval for the data to use when calculating the mean and std dev, which are used to eventually calculate the z-score.

### Usage and examples { #usage-and-examples } 

使用默认的z分数算法获取异常值：

```
GET / api / 36 / outlierDetection？ds = BfMAe6Itzgt＆ds = QX4ZTUbOt3a
  ＆ou = O6uvpzGd5pu＆ou = fdc6uOvgoji＆startDate = 2020-01-01＆endDate = 2020-12-31
```

使用特定算法和特定阈值获取异常值：

```
GET / api / 36 / outlierDetection？ds = BfMAe6Itzgt＆ds = QX4ZTUbOt3a
  ＆ou = O6uvpzGd5pu＆startDate = 2020-01-01＆endDate = 2020-12-31
  ＆algorithm = Z_SCORE＆threshold = 2.5
```

获取按z分数排序的异常值：

```
GET / api / 36 / outlierDetection？ds = BfMAe6Itzgt
  ＆ou = O6uvpzGd5pu＆startDate = 2020-01-01＆endDate = 2020-12-31
  ＆orderBy = Z_SCORE
```

获取前10个离群值：

```
GET / api / 36 / outlierDetection？ds = BfMAe6Itzgt
  ＆ou = O6uvpzGd5pu＆startDate = 2020-01-01＆endDate = 2020-12-31
  ＆maxResults = 10
```

获取具有定义间隔的离群值，以供在计算均值和标准差开发数据时使用的数据：

```
GET / api / 36 / outlierDetection？ds = BfMAe6Itzgt
  ＆ou = O6uvpzGd5pu＆startDate = 2020-01-01＆endDate = 2020-12-31
  ＆dataStartDate = 2018-01-01＆dataEndDate = 2020-12-31
```

使用最小-最大算法获取离群值：

```
GET / api / 36 / outlierDetection？ds = BfMAe6Itzgt＆ds = QX4ZTUbOt3a
  ＆ou = O6uvpzGd5pu＆ou = fdc6uOvgoji＆startDate = 2020-01-01＆endDate = 2020-12-31
  ＆algorithm = MIN_MAX
```

### 回应格式 { #response-format } 

支持以下响应格式。

| 格式 | API格式                                                   |
| ------ | ------------------------------------------------------------ |
| JSON格式   | `/ api / 36 / outlierDetection.json`或`Accept：application / json`（默认格式） |
| CSV    | `/ api / 36 / outlierDetection.csv`或`接受：application / csv`  |

响应包含以下字段：

| 领域      | 描述                                                  |
| ---------- | ------------------------------------------------------------ |
| 德         | 数据元素标识符。                                     |
| 取消命名     | 数据元素名称。                                           |
| 聚乙烯         | 期间ISO标识符。                                       |
| 欧         | 组织单位标识符。                                |
| ouName     | 组织单位名称。                                      |
| 可可        | 类别选项组合标识符。                      |
| cocName    | 类别选项组合名称。                            |
| 冠捷        | 属性选项组合标识符。                     |
| aocName    | 属性选项组合名称。                           |
| 价值      | 数据值。                                                  |
| 意思是       | 时间维度中数据值的平均值。                   |
| 标准差     | 标准偏差。                                          |
| 绝对值     | 对于z得分，与均值的绝对偏差。对于最小-最大，与最小或最大边界的绝对偏差。 |
| 分数     | Z分数。仅Z分数算法。                         |
| 下界 | 下边界。                                          |
| 上限 | 上限。                                          |
| 跟进   | 数据值是否标记为后续。                  |

The `mean`, `stdDev` and `zScore` fields are only present when `algorithm` is `Z_SCORE`.

响应将与此类似。 `元数据`部分包含请求和响应的元数据。 `outlierValues` 部分包含异常值。

```json
{
  "metadata": {
    "algorithm": "Z_SCORE",
    "threshold": 2.5,
    "orderBy": "MEAN_ABS_DEV",
    "maxResults": 10,
    "count": 10
  },
  "outlierValues": [
    {
      "de": "rbkr8PL0rwM",
      "deName": "Iron Folate given at ANC 3rd",
      "pe": "202011",
      "ou": "Pae8DR7VmcL",
      "ouName": "MCH (Kakua) Static",
      "coc": "pq2XI5kz2BY",
      "cocName": "Fixed",
      "aoc": "HllvX50cXC0",
      "aocName": "default",
      "value": 9000.0,
      "mean": 1524.5555,
      "stdDev": 2654.4661,
      "absDev": 7475.4444,
      "zScore": 2.8161,
      "lowerBound": -5111.6097,
      "upperBound": 8160.7208,
      "followUp": false
    },
    {
      "de": "rbkr8PL0rwM",
      "deName": "Iron Folate given at ANC 3rd",
      "pe": "202010",
      "ou": "vELbGdEphPd",
      "ouName": "Jimmi CHC",
      "coc": "pq2XI5kz2BY",
      "cocName": "Fixed",
      "aoc": "HllvX50cXC0",
      "aocName": "default",
      "value": 8764.0,
      "mean": 1448.0833,
      "stdDev": 2502.3031,
      "absDev": 7315.9166,
      "zScore": 2.9236,
      "lowerBound": -4807.6745,
      "upperBound": 7703.8412,
      "followUp": false
    }
  ]
}
```

### 约束与验证 { #constraints-and-validation } 

在查询验证期间，以下约束适用。每个验证错误都有一个对应的错误代码。

| 错误代码 | 信息                                                      |
| ---------- | ------------------------------------------------------------ |
| E2200      | 必须至少指定一个数据元素                  |
| E2201      | 必须指定开始日期和结束日期                    |
| E2202      | 开始日期必须早于结束日期                           |
| E2203      | 必须至少指定一个组织单位             |
| E2204      | 阈值必须为正数                          |
| E2205      | 最高结果必须为正数                        |
| E2206      | 最大结果超出了允许的最大限制：{d}               |
| E2207      | 数据开始日期必须早于数据结束日期                 |
| E2208      | 离群值检测期间遇到的非数字数据值 |

## 数据分析 { #webapi_data_analysis } 

用于执行数据分析和查找数据质量的多种资源
并提供验证问题。

**注意：**不建议使用此端点，该端点将在2.38中删除。请改用`outlierAnalysis`端点。

### 验证规则分析 { #webapi_data_analysis_validation_rules } 

要运行验证规则并检索违规：

    GET /api/dataAnalysis/validationRules

支持以下查询参数：



Table: Validation rule analysis query parameters

| 查询参数 | 描述 | 选项 |
|---|---|---|
| vrg | Validation rule group | ID |
| 欧 | 组织单位 | ID |
| 开始日期 | Start date for the timespan | 日期 |
| 结束日期 | End date for the timespan | 日期 |
| persist | Whether to persist violations in the system | false &#124; true |
| notification | Whether to send notifications about violations | false &#124; true |

样本输出：
```json
    [{
    "validationRuleId": "kgh54Xb9LSE",
    "validationRuleDescription": "Malaria outbreak",
    "organisationUnitId": "DiszpKrYNg8",
    "organisationUnitDisplayName": "Ngelehun CHC",
    "organisationUnitPath": "/ImspTQPwCqd/O6uvpzGd5pu/YuQRtpLP10I/DiszpKrYNg8",
    "organisationUnitAncestorNames": "Sierra Leone / Bo / Badjia / ",
    "periodId": "201901",
    "periodDisplayName": "January 2019",
    "attributeOptionComboId": "HllvX50cXC0",
    "attributeOptionComboDisplayName": "default",
    "importance": "MEDIUM",
    "leftSideValue": 10.0,
    "operator": ">",
    "rightSideValue": 14.0
}, {
    "validationRuleId": "ZoG4yXZi3c3",
    "validationRuleDescription": "ANC 2 cannot be higher than ANC 1",
    "organisationUnitId": "DiszpKrYNg8",
    "organisationUnitDisplayName": "Ngelehun CHC",
    "organisationUnitPath": "/ImspTQPwCqd/O6uvpzGd5pu/YuQRtpLP10I/DiszpKrYNg8",
    "organisationUnitAncestorNames": "Sierra Leone / Bo / Badjia / ",
    "periodId": "201901",
    "periodDisplayName": "January 2019",
    "attributeOptionComboId": "HllvX50cXC0",
    "attributeOptionComboDisplayName": "default",
    "importance": "MEDIUM",
    "leftSideValue": 22.0,
    "operator": "<=",
    "rightSideValue": 19.0
}]
```

### 基于标准差的离群分析 { #webapi_data_analysis_std_dev_outlier } 

根据平均值的标准偏差识别数据异常值
价值：

    GET /api/dataAnalysis/stdDevOutlier

支持以下查询参数：



Table: Standard deviation outlier analysis query parameters

| 查询参数 | 描述 | 选项 |
|---|---|---|
| 欧 | 组织单位 | ID |
| 开始日期 | Start date for the timespan | 日期 |
| 结束日期 | End date for the timespan | 日期 |
| ds | Data sets, parameter can be repeated | ID |
| standardDeviation | Number of standard deviations from the average | Numeric value |

### 基于最小值/最大值的离群值分析 { #webapi_data_analysis_min_max_outlier } 

要基于最小/最大值来识别数据离群值：

    GET /api/dataAnalysis/minMaxOutlier

支持的查询参数等于基于 *std dev 的异常值
上面描述的分析*资源。

### 后续数据分析 { #follow-up-data-analysis } 

要识别标记为后续的数据：

    GET /api/dataAnalysis/followup

At least one data set or data element, start date and end date or period, and at least one organisation unit must be defined.

支持以下查询参数。

| Parameter  | 描述                                                  | 强制的 | 选项（默认为默认）                   |
| ---------- | ------------------------------------------------------------ | --------- | ----------------------------------------- |
| 欧         | 组织单位，可以多次指定。          | 是的       | 组织单位标识符。             |
| ds         | 数据集，可以多次指定。                   | 不 [*]    | 数据集标识符。                      |
| 德         | 数据元素，可以多次指定。               | 不 [*]    | 数据元素标识符。                  |
| 开始日期  | 间隔的开始日期，以检查异常值。               | 不 [*]    | 日期（yyyy-MM-dd）。                        |
| 结束日期    | 检查异常值的时间间隔的结束日期。                 | 不 [*]    | 日期（yyyy-MM-dd）。                        |
| 聚乙烯         | ISO period ID.                                               | 不 [*]    | Period ISO ID.                        |
| peType     | ISO period.                                                  | 不 [*]    | Period ISO string.                        |
| 可可        | Category option combos, can be specified multiple times.     | 不        | Category option combo identifier.         |
| maxResults | 输出的最大限制。                                    | 不        | Integer, greater than zero. Default: 50.  |

[*]  You must specify either data sets with the `ds` parameter, which will include all data elements in the data sets, _or_ specify data elements with the `de` parameter.
     Equally, either `startDate` and `endDate` _or_ `period` must be specified.

The `startDate` and `endDate` parameters refer to the time interval for which you want to detect outliers.
If a period `pe` is provided instead the interval start and end is that of the period.

如果未提供选项组合`coc`，则考虑所有数值类型的数据元素。


## 数据的完整性 { #webapi_data_integrity } 

The data integrity capabilities of the data administration module are
available through the web API. This section describes how to run the
data integrity process and retrieve the results. The specific
details regarding each check are described in the user manual.

### Listing the available data integrity checks { #webapi_data_integrity_list }
A description of the available checks is returned by a request to:

    GET /api/dataIntegrity

```
[
    {
        "name": "data_elements_without_groups",
        "displayName": "Data elements lacking groups",
        "section": "Data Elements",
        "severity": "WARNING",
        "description": "Lists all data elements that have no data element groups",
        "issuesIdType": "dataElements",
        "isSlow": false
    }
]
```

The `name` member of the returned check elements is the identifier used for the
`checks` parameter to declare the set of checks to run.

> **Note**
> 
> Each check will indicate whether it may require significant time and resources to complete with the `isSlow` field. 
> Users should be cautious about running these
> checks on production systems as they could lead to decreased performance. 
> These checks can be run individually, but will 
> not be run unless specifically requested.

Checks are grouped semantically by the `section` member and categorised in 
one of four `severity` levels:

| Severity | 描述                                                                                                                   |
| -------- |-------------------------------------------------------------------------------------------------------------------------------|
| INFO     | Indicates that this is for information only.                                                                                  |
| WARNING  | A warning indicates that this may be a problem, but not necessarily an error. It is however recommended to triage these issues. |
| SEVERE   | An error that should be fixed but which may not necessarily lead to the system not functioning.                               |
| CRITICAL | An error that must be fixed and which may lead to end-user error or system crashes.                                           |

The available checks can be filtered using the `checks` parameter.

    GET /api/dataIntegrity?checks=<pattern1>,<pattern2>

One or more exact names or patterns using `*` as a wildcard can be provided.

Additional results can be filtered using a `section` parameter.

    GET /api/dataIntegrity?section=Categories

The `section` filter will return all exact matches which have the specified section. 

Furthermore, to filter (select) only checks marked as `isSlow` use `slow=true`,

    GET /api/dataIntegrity?slow=true

or to filter (select) only checks that are not performed via database query 
(programmed checks) use `programmatic=true`:

    GET /api/dataIntegrity?programmatic=true

The `slow`, `programmatic` and `section` filters can be combined in which case
all conditions must be met.

### Running data integrity summaries { #webapi_data_integrity_run_summary }

Since version 2.38, data integrity checks have two levels of specificity: 
- a `summary` level that provides an overview of the number of issues
- a `details` level that provides a list of issues pointing to individual data integrity violations.

To trigger a summary analysis for a set of checks run:

    POST /api/dataIntegrity/summary?checks=<name1>,<name2>

This triggers a job that runs the check(s) asynchronously. Individual check results
will be returned to the application cache as soon as the check has completed.

Alternatively the list of checks can also be given as BODY of the POST request.
This can be useful if the list becomes to long to be used in the URL.

To fetch the data integrity summary of the triggered check(s) use:

    GET /api/dataIntegrity/summary?checks=<name1>,<name2>

When the `checks` parameter is omitted, all checks are fetched from the server cache.

The response is a "map" of check results, one for each check that has completed already.
This information is cached for one hour or until the check is rerun.

To wait for the summary to be available in the cache a `timeout` in milliseconds can be added:

    GET /api/dataIntegrity/summary?checks=<name1>,<name2>&timeout=500

An example of a summary response could look like: 
```json
{
  "<name1>": {
    "name": "<name1>",
    "displayName": "<displayName1>",
    "startTime": "2023-01-11T06:12:56.436",
    "finishedTime": "2023-01-11T06:12:57.021",
    "section": "...",
    "severity": "WARNING",
    "description": "...",
    "count": 12,
    "percentage": 2.3
  },
  "<name2>": {
    "name": "<name2>",
    "displayName": "<displayName2>",
    "startTime": "2023-01-11T06:12:57.345",
    "finishedTime": "2023-01-11T06:12:58.007",
    "section": "...",
    "severity": "WARNING",
    "description": "...",
    "count": 4,
    "percentage": 5.1
  }
}
```

Each summary response will contain the `name`, `section`, `severity`, 
`description` and optionally  an `introduction` and `recommendation`.  
Each summary contains the number of issues found in the `count` field. When possible,
an optional `percentage` field will provide the percentage of objects with data
integrity issues when compared to all objects of the same type.
The `startTime` field indicates when the check was initiated. Using the `finishedTime`
the duration which was required to execute the check can be calculated.

Should a check analysis fail due to programming error or unforeseen data inconsistencies
both the summary and the details will have an `error` field describing the error that occurred.
The `count` of any checks which failed will be set to -1. 
No `percentage` will be returned in such cases.

```json
{
  "<name1>": {
    "name": "<name1>",
    "displayName": "<displayName1>",
    "finishedTime": "2022-02-15 14:55",
    "section": "...",
    "severity": "WARNING",
    "description": "...",
    "error": "what has happened",
    "issues": []
  }
}
```

> **Note**
> 
> Each metadata check is run asynchronously on the server.  Results
> will be returned as soon as each check completes. The safest way to ensure 
> that you have retrieved the latest set of results which has been 
> requested is to compare the timestamp of when the request was made
> with the `finishedTime` in the response.

To get a list of the names of checks that are currently being performed by the 
server use:

    GET /api/dataIntegrity/summary/running

To get a list of the names of checks for which results are available already use:

    GET /api/dataIntegrity/summary/completed


### Running data integrity details { #webapi_data_integrity_run_details }

To run a selection of details checks first trigger them using a  `POST` request:

    POST /api/dataIntegrity/details?checks=<name1>,<name2>

Similar to the summary the list of checks can also be given as the POST body.

Then fetch the results from the cache using:

    GET /api/dataIntegrity/details?checks=<name1>,<name2>&timeout=500

When the `checks` parameter is not provided,  all checks which 
have not been marked as `isSlow` will be scheduled to be run on the server.

Omitting the `timeout` will not wait for results to be found in the cache, 
but instead not have a result for the requested check.

The `/details` response returns a map similar to the `summary`, but does not contain
a `count` or `percentage`. Instead, a list of `issues` is returned.

```json
{
  "<name1>": {
    "name": "<name1>",
    "displayName": "<displayName1>",
    "startTime": "2023-01-11T06:12:56.436",
    "finishedTime": "2023-01-11T06:12:57.021",
    "section": "...",
    "severity": "WARNING",
    "description": "...",
    "issuesIdType": "<object-type-plural>",
    "isSlow": false,
    "issues": [{
      "id": "<id-or-other-identifier>",
      "name": "<name-of-the-id-obj>",
      "comment": "optional plain text description or hint of the issue",
      "refs": ["<id1>", "<id2>"]
    }]
  },
  "<name2>": {
    "name": "<name2>",
    "displayName": "<displayName2>",
    "startTime": "2023-01-11T06:12:57.345",
    "finishedTime": "2023-01-11T06:12:58.007",
    "section": "...",
    "severity": "WARNING",
    "description": "...",
    "issuesIdType": "<object-type-plural>",
    "isSlow": false,
    "issues": []
  }
}
```
Each issue will always have `id` and `name` members.  Often the `issuesIdType`
is available to indicate the type of objects the `id` refers to. If the 
`issuesIdType` is not available, the `id` often is not available either and the
`name` is used for an aggregate key of an issue that has no object equivalent.

The `comment` and `refs` fields are optional for each issue.
A `comment` may provide more context or
insight into why this particular issue is regarded to be a data integrity problem. 
The `refs` list may also give the identifiers of other objects that contributed to the violation.
The `finishedTime` field shows when the particular check finished processing on the server.
The cache will store the result of each completed check for one hour.

> **Tip**
>
> A set of checks can also be specified using wild-cards. To include all 
> checks with _element_ in the name use `checks=*element*`. Like full names 
> such patterns can be used in a comma-separated list and be mixed with full 
> names as well. Duplicates will be eliminated. 
> Also a check can be given by its code. A code consists of the first letters
> of each word in the name as upper case letter. 
> For example, `orgunits_invalid_geometry` has the code `OIG`.

Similar to the summary a set of names of the currently performed and the
already completed details checks can be obtained using:

    GET /api/dataIntegrity/details/running
    GET /api/dataIntegrity/details/completed

### Custom Data Integrity Checks { #custom_data_integrity_checks } 

Users of DHIS2 can now create and supply their own Data Integrity Checks. This can be useful if users
want to avail of this functionality and extend upon the supplied set of core data integrity checks.

> **Tip**
> 
> Users are also encouraged to share their custom checks with others by opening a pull request in the 
> [dhis2-core](https://github.com/dhis2/dhis2-core) repository containing their `.yaml` file(s).
> Please select `platform-backend` as reviewer to put the PR on our radar early on. The team will 
> take care of checking and linking the check correctly, so it becomes part of the provided suite of 
> checks with the next release. 

An example of a custom check could be for determining if certain users are members of specific user groups.
This type of check would be very specific to an implementation, and not generally applicable across all installs.
These types of metadata checks can be used to extend the default checks which are included with DHIS2.

Custom checks can be implemented by satisfying the following requirements, each of which we will go into detail:
- Supplying your own list of custom data integrity checks in a list file named `custom-data-integrity-checks.yaml`
 in your `DHIS2_HOME` directory
- Having a directory named `custom-data-integrity-checks` in your `DHIS2_HOME` directory
- Supplying your valid custom data integrity check yaml files

#### Custom Data Integrity Check List File { #custom-data-integrity-check-list-file } 

DHIS2 will only try to load data integrity files when they are needed. e.g. when making a call to view all
data integrity checks:

    GET /api/dataIntegrity

DHIS2 will look for a file named `custom-data-integrity-checks.yaml` in your `DHIS2_HOME` directory when loading
data integrity files. If you are not using custom checks and the file is not present, a warning log like this will
be present:

```text
08:29:57.729  WARN o.h.d.d.DataIntegrityYamlReader: Failed to load data integrity check from YAML. Error message `{DHIS2_HOME}/custom-data-integrity-checks.yaml (No such file or directory)
```

If you are implementing custom data integrity checks then this file must be present. To see what the core data integrity checks
file looks like as an example, check out [this file](https://github.com/dhis2/dhis2-core/blob/master/dhis-2/dhis-services/dhis-service-administration/src/main/resources/data-integrity-checks.yaml).


The `custom-data-integrity-checks.yaml` file should list all of your custom data integrity checks.
As an example, it could look something like this:

```yaml
checks:
  - categories/my_custom_check.yaml
  - users/my_user_group_check.yaml
  - base_check.yaml
```

Check names in this file can be preceded with a directory name for logical grouping. From the 3 example checks listed 
above, the directory structure should look like this:

```
├── DHIS2_HOME
│   ├── dhis.conf
│   ├── custom-data-integrity-checks.yaml
│   ├── custom-data-integrity-checks
│   │   ├── categories
│   │   │   ├── my_custom_check.yaml
│   │   ├── users
│   │   │   ├── my_user_group_check.yaml
│   │   ├── base_check.yaml
```

#### Name and Code constraints { #name-and-code-constraints } 

Each data integrity check `name` and `code` must be unique. If there are any clashes then the violating custom
check will not be loaded.

> **Note**
>
> System data integrity checks are always loaded first. Any name or code clashes resulting from
> custom checks will not affect these core system checks.

An example data integrity check yaml file is located [here](https://github.com/dhis2/dhis2-core/blob/master/dhis-2/dhis-services/dhis-service-administration/src/main/resources/data-integrity-checks/orgunits/orgunits_orphaned.yaml)
for reference. Note the `name` property.

The data integrity `code` is calculated dynamically by using the first letter of each word in the `name`. Some examples:

| 名称                   | 码 |
|------------------------|------|
| my_custom_check        | MCC  |
| my_second_custom_check | MSCC |
| another_custom_check   | ACC  |

If there is a `name` clash, a warning log like this will be present:
```text
09:48:43.138  WARN o.h.d.d.DefaultDataIntegrityService: Data Integrity Check `my_custom_check` not added as a check with that name already exists
```

If there is a `code` clash, a warning log like this will be present:
```text
09:48:43.138  WARN o.h.d.d.DefaultDataIntegrityService: Data Integrity Check `my_custom_check` not added as a check with the code `MCC` already exists
```

#### Data Integrity Check Schema { #data-integrity-check-schema } 

A data integrity check file must comply with this [JSON schema](https://github.com/dhis2/dhis2-core/blob/master/dhis-2/dhis-services/dhis-service-administration/src/main/resources/data-integrity-checks/integrity_check_schema.json).
If a check does not comply with the schema then a warning like this will be present:
```text
09:48:43.136  WARN o.h.d.d.DataIntegrityYamlReader: JsonSchema validation errors found for Data Integrity Check `categories/my_custom_check.yaml`. Errors: [$.name: is missing but it is required]
```

Any schema violations must be fixed before that check can be loaded and used.

If a data integrity check file contains invalid yaml then a warning log like this could be present:
```text
10:30:37.858  WARN o.h.d.d.DataIntegrityYamlReader: JsonSchema validation errors found for Data Integrity Check `my_custom_check.yaml`. Errors: [$: string found, object expected]
```

To view and use the custom checks please refer to the main [Data Integrity section](#webapi_data_integrity)

> **Note**
>
> It is recommended to follow any naming and format conventions seen in the provided examples above when implementing
> your own custom checks to help avoid any issues

#### Data Integrity File { #data-integrity-file } 

Details of the data integrity check yaml file, taken from the JSON schema file

| property        | required | info                                                                                                                          |
|-----------------|----------|-------------------------------------------------------------------------------------------------------------------------------|
| 名称            | yes      | unique name of the check                                                                                                      |
| 描述     | yes      | 描述                                                                                                                   |
| section         | yes      | used for logical grouping of checks e.g. categories, users                                                                    |
| section_order   | yes      | the order of the check when displayed in the UI                                                                               |
| summary_sql     | yes      | an SQL query which should return a single result which represents the total count of issues                                   |
| details_sql     | yes      | an SQL query which should return a list of identified objects from this particular issue. Should return at least uid and name |
| details_id_type | yes      | a short string which identifies the section of the details SQL                                                                |
| severity        | yes      | level of severity of the issue. One of [INFO, WARNING, SEVERE, CRITICAL]                                                      |
| introduction    | yes      | outlining the objective of the check                                                                                          |
| recommendation  | yes      | outlining how to resolve identified issues                                                                                    |

### Example custom data integrity check { #example-custom-data-integrity-check } 


An example of a custom check could be for determining if users have an email. Emails are useful to be
able to communicate with users and sent them notifications, as well as password recovery. So, in some
instllations of DHIS2, it could be a policy that all users should have emails. An example of this type
of custom check is shown below.

```
---
name: users_should_have_emails
description: Users should have emails.
section: Users
section_order: 6
summary_sql: >-
  WITH users_no_email as (
  SELECT uid,username from
  userinfo where email IS NULL)
  SELECT COUNT(*) as value,
  100*COUNT(*) / NULLIF( ( select COUNT(*) from userinfo), 0) as percent
  from users_no_email;
details_sql: >-
  WITH users_no_email as (
  SELECT uid,username from
  userinfo where email IS NULL)
  SELECT uid,username as from users_no_email;
severity: WARNING
introduction: >
  Users should have defined emails. This is important for password recovery and to be able
  to send notifications to users.
recommendation: >
  Make sure that all users have defined emails.
details_id_type: users
```

More examples of different types of metadata integrity checks can be found in the DHIS2 source code [here](https://github.com/dhis2/dhis2-core/tree/master/dhis-2/dhis-services/dhis-service-administration/src/main/resources/data-integrity-checks).

## 完整的数据集注册 { #webapi_complete_data_set_registrations }

本节是关于数据集的完整数据集注册。一种
注册标记作为完全捕获的数据集。

### 完成数据集 { #webapi_completing_data_sets }

本节说明如何将数据集注册为完整。这是
通过与 *completeDataSetRegistrations* 交互实现
资源：

    GET /api/33/completeDataSetRegistrations

端点支持*POST*方法注册数据集
完成。端点在功能上非常类似于
*dataValueSets* 端点，支持批量导入完整
注册。

支持导入 *XML* 和 *JSON* 格式的有效负载。这
这个有效负载的基本格式，在这个例子中以 *XML* 给出，就像
所以：

```xml
<completeDataSetRegistrations xmlns="http://dhis2.org/schema/dxf/2.0">
  <completeDataSetRegistration period="200810" dataSet="eZDhcZi6FLP" 
    organisationUnit="qhqAxPSTUXp" attributeOptionCombo="bRowv6yZOF2" storedBy="imported"/>
  <completeDataSetRegistration period="200811" dataSet="eZDhcZi6FLP" 
    organisationUnit="qhqAxPSTUXp" attributeOptionCombo="bRowv6yZOF2" storedBy="imported"/>
</completeDataSetRegistrations>
```

*storedBy* 属性是可选的（因为它是
完整的注册对象）。您还可以选择设置
*date* 属性（注册时间）作为属性。是时候了
未设置，将使用当前时间。

导入过程支持以下查询参数：



Table: Complete data set registrations query parameters

| Parameter | Values | 描述 |
|---|---|---|
| dataSetIdScheme | id &#124; name &#124; code &#124; attribute:ID | Property of the data set to use to map the complete registrations. |
| orgUnitIdScheme | id &#124; name &#124; code &#124; attribute:ID | Property of the organisation unit to use to map the complete registrations. |
| attributeOptionComboIdScheme | id &#124; name &#124; code &#124; attribute:ID | Property of the attribute option combos to use to map the complete registrations. |
| 方案 | id &#124; name &#124; code &#124; attribute:ID | Property of all objects including data sets, org units and attribute option combos, to use to map the complete registrations. |
| preheatCache | false &#124; true | Whether to save changes on the server or just return the import summary. |
| dryRun | false &#124; true | Whether registration applies to sub units |
| importStrategy | CREATE &#124; UPDATE &#124; CREATE_AND_UPDATE &#124; DELETE | Save objects of all, new or update import status on the server. |
| skipExistingCheck | false &#124; true | Skip checks for existing complete registrations. Improves performance. Only use for empty databases or when the registrations to import do not exist already. |
| async | false &#124; true | Indicates whether the import should be done asynchronous or synchronous. The former is suitable for very large imports as it ensures that the request does not time out, although it has a significant performance overhead. The latter is faster but requires the connection to persist until the process is finished. |

The `idScheme`, `dataSetIdScheme`, `orgUnitIdScheme`, `attributeOptionComboIdScheme`, 
`dryRun` and `strategy` (note the dissimilar naming to parameter `importStrategy`) 
can also be set as part of the payload.
In case of XML these are attributes, in case of JSON these are members in the
`completeDataSetRegistrations` node.

例如：
```xml
<completeDataSetRegistrations xmlns="http://dhis2.org/schema/dxf/2.0"
      orgUnitIdScheme="CODE">
    <completeDataSetRegistration period="200810" dataSet="eZDhcZi6FLP"
    organisationUnit="OU_559" attributeOptionCombo="bRowv6yZOF2" storedBy="imported"/>
</completeDataSetRegistrations>
```

Should both URL parameter and payload set a scheme the payload takes precedence. 

### 读取完整的数据集注册 { #webapi_reading_complete_data_sets } 

本节说明如何检索数据集完整性
注册。我们将使用 *completeDataSetRegistrations*
资源。要使用的查询参数如下：



Table: Data value set query parameters

| Parameter | 描述 |
|---|---|
| dataSet | Data set identifier, multiple data sets are allowed |
| period | Period identifier in ISO format. Multiple periods are allowed. |
| 开始日期 | Start date for the time span of the values to export |
| 结束日期 | End date for the time span of the values to export |
| created | Include only registrations which were created since the given timestamp |
| createdDuration | Include only registrations which were created within the given duration. The format is <value\><time-unit\>, where the supported time units are "d", "h", "m", "s" *(days, hours, minutes, seconds).* The time unit is relative to the current time. |
| orgUnit | Organisation unit identifier, can be specified multiple times. Not applicable if orgUnitGroup is given. |
| orgUnitGroup | Organisation unit group identifier, can be specified multiple times. Not applicable if orgUnit is given. |
| children | Whether to include the children in the hierarchy of the organisation units |
| limit | The maximum number of registrations to include in the response. |
| 方案 | Identifier property used for meta data objects in the response. |
| dataSetIdScheme | Identifier property used for data sets in the response. Overrides idScheme. |
| orgUnitIdScheme | Identifier property used for organisation units in the response. Overrides idScheme. |
| attributeOptionComboIdScheme | Identifier property used for attribute option combos in the response. Overrides idScheme. |
The `dataSet` and `orgUnit` parameters can be repeated in order to include multiple data sets and organisation units.

The `period`, `startDate`,  `endDate`, `created` and `createdDuration` parameters provide multiple ways to set the time dimension for the request, thus only
one can be used. For example, it doesn't make sense to both set the start/end date and to set the periods.

请求示例如下所示：

```bash
GET /api/33/completeDataSetRegistrations?dataSet=pBOMPrpg1QX
  &startDate=2014-01-01&endDate=2014-01-31&orgUnit=YuQRtpLP10I
  &orgUnit=vWbkYPRmKyS&children=true
```

您可以获得 *xml* 和 *json* 格式的响应。你可以指出
通过 *Accept* HTTP 标头，您更喜欢哪种响应格式
在上面的例子中。对于 xml，您使用 *application/xml*；对于 json 你
使用*应用程序/json*。

### 未完成的数据集 { #webapi_uncompleting_data_sets } 

本节说明如何取消注册数据的完整性
放。要取消完成数据集，您将与
completeDataSetRegistrations 资源：

    GET /api/33/completeDataSetRegistrations

此资源支持*DELETE* 取消注册。以下查询
支持参数：



Table: Complete data set registrations query parameters

| 查询参数 | 需要 | 描述 |
|---|---|---|
| ds | 是的 | Data set identifier |
| 聚乙烯 | 是的 | 期间标识符 |
| 欧 | 是的 | Organisation unit identifier |
| cc | No (must combine with cp) | Attribute combo identifier (for locking check) |
| cp | No (must combine with cp) | Attribute option identifiers, separated with ; for multiple values (for locking check) |
| multiOu | No (default false) | Whether registration applies to sub units |

