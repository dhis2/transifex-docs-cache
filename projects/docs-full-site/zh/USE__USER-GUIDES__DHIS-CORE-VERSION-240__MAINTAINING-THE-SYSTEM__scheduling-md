---
edit_url: "https://github.com/dhis2/dhis2-docs/blob/2.40/src/user/scheduling.md"
revision_date: '2022-03-19'
tags:
- DHIS 核心 2.40 版
- 使用
---

# 排程 { #scheduling } 

计划程序是用于管理DHIS2中的后台作业的应用程序。
后台作业可以执行许多任务，例如运行分析，
同步数据和元数据，或发送推送分析报告。的
应用程序提供了创建，修改和删除此类作业的功能。

调度程序与DHIS2捆绑在一起，可以通过应用程序进行访问
菜单。

![The start page of the Scheduler app](resources/images/scheduler/overview.png)

The start page of the Scheduler app shows an overview of existing jobs.
By default, pre-defined system jobs are hidden. To view these, click
*Include system jobs in list* in the top right corner.

When you create or modify a job, it will be scheduled according to
the selected schedule. To run a job on demand, go to the job list,
click the "Actions" button of the job you want to run and click
"Run manually". This action is only available for enabled jobs.

## 创造工作 { #scheduling_create_job } 

1.  Open the **Scheduler** app and click the "New job" button in the top
    右上角。

1.  Choose a suitable **Name** for the new job.

1.  Select the **Job type** you want to schedule using the
    下拉式菜单。

1.  Select a schedule for the job. Each job type has its own scheduling type,
    either **Cron** scheduling or **Delay** scheduling.

    1.  For **Cron** scheduled job types you can set a schedule using the 
        [Spring scheduling](https://docs.spring.io/spring/docs/current/javadoc-api/org/springframework/scheduling/support/CronSequenceGenerator.html) 
        syntax. You can also select a predefined **Cron expression** by clicking
        "Choose from preset times". This schedule will only start a new job run
        if the previous job run has finished, to prevent the system from spawning
        too many jobs.

    1.  For **Delay** scheduled jobs you can set a delay in seconds. Unlike the
        **Cron** scheduled jobs, these jobs aren't executed according to a set 
        schedule, but with a specific delay in between job runs. The delay timer
        starts when a job ends, starting a new job run when the delay timer reaches
        zero. This will continue as long as the job is enabled.

1.  If the job type is customizable, a **Parameters** section will appear below
    the scheduling settings. These additional options specify the details of the
    scheduled job, and will vary depending on the job type.

1.  Press the **Save** button to confirm the job creation. On successful job
    creation you will be redirected to the job overview, where the newly
    created job will now be listed.

![Creating a new scheduler job](resources/images/scheduler/add_new_job.png)

Newly created jobs are enabled by default.

## Editing a job { #scheduling_configure_job } 

With the proper permissions, you can modify the details of user-created
jobs. To quickly enable or disable a user created job from running, use the
switches in the **On/off** column on the landing page of the Scheduler app.
Note that system jobs are always enabled and cannot be disabled.

Further editing of user jobs:

1.  Click the "Actions" button of the job you want to edit and click "Edit" (only
    user jobs can be edited).

1.  When done editing, press the **Save** button to persist the changes.

## 删除工作 { #dataAdmin_scheduler_delete } 

1.  Click the "Actions" button of the job you want to delete and click "Delete"
    (only user jobs can be deleted).

1.  在弹出窗口中再次按** Delete **进行确认。

User jobs can also be deleted from the editing screen.

![Deleting a scheduler job](resources/images/scheduler/delete_job.png)

## 工作类型 { #job-types } 

以下部分描述了各种作业类型。

### Disable Inactive Users { #scheduling_disable_inactive_users } 
Users that have not been active - not logged in - for a number of months can automatically be disabled.
Select the number of inactive months as the job parameter.
All users that have not logged in for that number of months or longer will be disabled by the job.
Disabled users will no longer be able to log into the system.

The _Reminder days before_ parameter can be set to send a reminder email to
those users the specified number of days before their account is due to expire.
If users do not log in further reminder emails are sent each halving the 
previous number of days. For example if the number of days is set to 7 the 
first email is sent 7 days in advance, the second 3 days and the third and 
last 1 day in advance.
If the value is not set (blank) no reminder is sent.


### 资源表 { #scheduling_resource_table } 

资源表作业负责生成和更新资源数据库表。这些表由DHIS 2中的各个组件使用，旨在简化针对数据库的查询。

请注意，当指定任何分析表作业时，资源表可以是该过程的一部分，并且也不必指定资源表作业。

### 分析表 { #scheduling_analytics_table } 

分析表作业负责生成和更新分析表。分析表用作DHIS2中数据分析查询的基础。仪表板，可视化工具和地图等应用程序通过DHIS2分析API从这些表中检索数据，并且必须对其进行更新才能使分析数据可用。您可以安排此过程通过分析表作业类型定期运行。

默认情况下，分析表作业将填充所有年份和数据元素的数据。可以使用以下参数：

- **过去几年：**为其填充分析表的最后几年的数量。例如，如果您指定2年，则该过程将更新最近两年的数据，而不更新较旧的数据。此参数对减少过程完成所需的时间很有用，并且如果旧数据没有更改，并且需要更新最新数据，则该参数非常适用。
- **跳过资源表：**在分析表更新过程中跳过资源表。这减少了过程完成所需的时间，但导致元数据的更改未反映在分析数据中。
- **跳过表类型：**跳过一种或多种分析表类型。这减少了过程完成所需的时间，但导致这些数据类型未在分析数据中更新。

### 连续分析表 { #scheduling_continuous_analytics_table } 

分析表作业负责生成和更新分析表。分析表用作DHIS2中数据分析查询的基础。仪表板，可视化工具和地图等应用程序通过DHIS2分析API从这些表中检索数据，并且必须对其进行更新才能使分析数据可用。您可以安排此过程通过分析表作业类型定期运行。

连续分析表作业基于两个阶段：

- _最新更新：_最新数据的更新，其中最新是指自上次更新最新数据或完整数据以来已添加，更新或删除的数据。此过程将经常发生。
- _完整更新：_多年来所有数据的更新。此过程每天进行一次。

连续分析表作业将经常更新最新数据。最新数据处理使用特殊的数据库分区，该分区仅用于保存最新数据。由于数据量相对较少，因此可以快速刷新此分区。分区的大小将增加，直到执行完全更新。每天一次，将更新所有年份的所有数据。这将清除最新的分区。

默认情况下，分析表作业将填充所有年份和数据元素的数据。可以使用以下参数：

- **一天中的完整更新时间：**一天中的完整更新将在这一小时完成。例如，如果您指定1，则将在凌晨1点执行完整更新。
- **过去几年：**为其填充分析表的最后几年的数量。例如，如果您指定2年，则该过程将更新最近两年的数据，而不更新较旧的数据。此参数对减少过程完成所需的时间很有用，并且如果旧数据没有更改，并且需要更新最新数据，则该参数非常适用。
- **跳过资源表：**在分析表更新过程中跳过资源表。这减少了过程完成所需的时间，但导致元数据的更改未反映在分析数据中。

### Tracker search optimization { #scheduling_tracker_search_optimization } 

The tracker search optimization job is responsible for generating and updating the trigram indexes for relevant tracked entity attributes. Trigram indexes improve the performance of searching tracked entity instances based on specific tracked entity attribute values. The usefulness of trigram indexes depends on whether the tracked entity attribute is configured as unique or if they are configured as searchable (when connected to program/tracked entity type). You can configure the job to choose which tracked entity attributes should be trigram indexed. The job also takes care of deleting any obsolete indexes that have been created earlier but are no more required due to change in metadata configuration. 

The following parameters are available:

- **Attributes:** The list of attributes that needs a trigram index created. For each attribute, a partial trigram index will be created. As an example, if you specify "firstname" and "lastname" attribute, the process will create two separate trigram indexes for the corresponding attributes "firstname" and "lastname". Note that, if the attribute provided in this parameter is not indexable (either because they are not unique or not searchable), such attributes are simply ignored by the process and no trigram index will be created for them. 
- **Skip index deletion:** Skip obsolete index deletion during the trigram index process. If set to true, indexes that are deemed obsolete will not be deleted.

### 数据同步 { #scheduling_data_sync } 

DHIS2 provides synchronisation of data between remotely distributed
instances and a central instance of DHIS2. This can be useful e.g. when
you have deployed multiple stand-alone instances of DHIS2 which are
required to submit data values to a central DHIS2 instance. Both tracker
data and aggregate data synchronization is supported.

这些是启用数据同步的步骤：

- 转到“同步设置”，输入远程服务器URL，
  用户名和密码。按TAB按钮自动保存
  新密码。刷新页面并检查填充值
  仍然存在。请注意，密码字段将在之后为空
  刷新，因为此值是加密的，所以您可以考虑使用它
  已保存。

- 使用Scheduler应用程序，使用“事件程序
  数据同步”和/或“跟踪器程序数据同步”作业类型。请确保
  完成后启用。 （注意：如果“程序数据同步”
  以前版本中可用的作业是在Scheduler应用中设置的，
  它被两个新的作业“事件程序数据同步”自动替换
  和“ Tracker Programs Data Sync”具有相同的设置。 ）

数据同步功能的某些方面需要注意：

- 本地DHIS2实例将存储用户帐户的密码
  在本地数据库中加密的远程实例上。遥控器
  帐户在传输数据时用于身份验证。对于
  security purposes make sure you set the _encryption.password_
  configuration parameter in _hibernate.properties_ to a strong
  密码。

- 强烈建议在SSL / HTTPS上部署远程服务器，因为
  用户名和密码使用基本格式以明文形式发送
  身份验证，并且可能被攻击者拦截。

- 数据同步使用数据元素的UID属性，
  类别选项组合和组织单位以匹配
  元数据。因此，同步取决于这三个
  在本地和远程实例上协调元数据对象
  为了工作正常。

- DHIS2首次运行同步作业时，它将包括
  任何可用数据。随后的同步作业将仅
  包括自上次成功工作以来添加和更改的数据。一种
  仅当所有数据都被认为是成功的同步作业
  已成功保存在远程服务器上（任何数据成功
  无论作业是否同步，同步后的邮件都会保留在接收实例上
  最终失败）。这项工作是否成功可以
  由中央服务器返回的导入摘要决定。

- 初始同步作业可能需要花费大量时间
  时间，可能会使实例变慢，具体取决于多少
  数据正在同步。最好配置
  在线用户很少时运行作业，然后稍后更改
  to your own preference. If you do not want or need to synchronize all 
  数据，则有可能<a href="#skip_changed_before">跳过
  一些数据正在同步</a>。

  When DHIS2 synchronizes tracker data, it determines the set of data
  to synchronize based on the last time it was synchronized. Each of
  跟踪的实体实例和事件具有自己的记录
  when they where last successfully synchronized.

- 系统将根据规则集启动同步作业
  在作业的配置中。如果同步作业开始
  没有与远程服务器的连接时，它将重试
  至中止之前的三倍。作业将在
  计划的时间。

- 服务器分别处理每组程序，这意味着一个
  一组程序可以成功同步，而另一组
  失败。一个人的失败或成功不会影响另一个人，因为
  最后的成功同步时间将单独跟踪
  如前所述，每个项目。

- TrackedEntityInstances的属性（TrackedEntityAttribute）
  和ProgramStages的数据元素（ProgramStageDataElement）
  启用了“跳过同步”选项的用户将不会
  已同步。此功能使您可以决定不同步
  一些敏感或不相关的数据，并将其仅保留在本地。

- The authority `Ignore validation of required fields in Tracker and Event Capture`
  (`F\_IGNORE\_TRACKER\_REQUIRED\_VALUE\_VALIDATION`) should be used when
  要求某些强制属性/数据元素
  同时具有“跳过同步”属性。
  这样的设置将导致中央服务器上的验证失败
  因为给定的属性/数据元素将不会出现在
  有效载荷。

  具有此权限的用户验证不会失败。的
  应该在中央服务器上将权限分配给用户，
  将用于同步作业。

- In specific cases, **the initial synchronization of all the data can be undesirable**; 
  例如，当本地实例上的数据库是
  数据库存在于中央实例上，或者在不推荐的情况下
  synchronize old data in favor of initial synchronization taking less 
  时间。

  * syncSkipSyncForDataChangedBefore * SettingKey可用于跳过
  所有数据（数据值，事件和跟踪器）的同步
  程序数据，完整的数据集注册）*最后
  changed before the specified date*. The `SettingKey` is used in the 
  synchronization job all the time. Therefore, if you need to synchronize 
  the old data, you should change the `SettingKey`.

- Both Tracker Programs and Event Programs synchronization job supports 
  分页是为了避免超时并处理不稳定的网络。
  “事件程序数据同步”作业的默认页面大小设置为60。
  “跟踪程序数据同步”作业的默认页面大小设置为20。

  如果默认值不符合您的目的，则可以指定自己的页面大小
  通过参数，特别是Scheduler应用中的同步作业。

### 元数据同步调度 { #scheduling_metadata_sync } 

DHIS2提供了用于同步远程元数据的功能
实例到DHIS2的本地实例。当您有
部署了DHIS2的多个独立实例，您需要创建
所有本地实例中的元数据都类似于中央DHIS2
实例。

这些是启用元数据同步的步骤：

- 转到设置\>同步，输入远程服务器URL，
  用户名和密码，然后单击保存。

- 转到元数据管理\>计划。在元数据下
  将同步设置策略设置为“已启用”，然后选择时间段和
  单击开始。

元数据同步功能的某些方面需要注意：

- 本地DHIS2实例将存储用户帐户的密码
  远程实例在其数据库中的位置。远程用户帐户是
  用于传输/下载数据时的身份验证。对于
  security purposes make sure you set the _encryption.password_
  configuration parameter in _hibernate.properties_ to a strong
  密码。

- 强烈建议在SSL / HTTPS上部署远程服务器，因为
  用户名和密码使用基本格式以明文形式发送
  身份验证，并且可能被攻击者拦截。

- 另外，请确保远程用户没有ALL权限，
  相反，只需创建具有F \ _METADATA \ _MANAGE权限的用户，即可
  即使这些细节被黑客拦截，
  完全控制远程系统。

- 元数据同步依赖于基础导入层。
  每个元数据版本都是两个给定之间的元数据导出
  时间戳记。每次同步元数据版本都是尝试导入
  该元数据快照到本地实例中。的同步
  版本是增量的。本地实例将尝试下载
  来自中央实例的元数据版本一个接一个。
  无法同步特定的元数据版本将无法同步
  进行进一步的版本。发生故障时，适当
  必须对中央的元数据进行更改，以确保
  错误得到解决。元数据配置至关重要，
  user should be careful while rolling out the updates to the
  生产。始终建议在
  确保元数据版本及其完整性的地方
  此后的影响。本地实例将同步来自
  第一版，以便保持和谐以及本地和中央
  实例将正常工作。

- 系统将在计划的时间尝试同步。如果
  本地或远程服务器没有可用的Internet
  当时的连接，同步将被中止，并且
  按照尝试次数中所述的重试次数重新尝试
  _dhis.conf_ file.

- 您可以看到上一次与远程服务器成功同步的时间
  调度屏幕中“最后成功”标签旁边的服务器。

### Predictor { #scheduling_predictor } 

This runs selected predictors and/or predictor groups.

The relative start and end parameters determine the periods in
which data will be predicted, corresponding to the date on which the
predictor job is run:

- **Relative start** counts the days from the job date to the
earliest date on which a predicted period may start. It can be positive
or negative. For example, a value of 3 means predict into periods that
start at least 3 days after the predictor run. A value of -3 means
predict into periods that start at least 3 days before the predictor run.

- **Relative end** counts the days from the job date to the
latest date on which a predicted period may end. It can be positive
or negative. For example, a value of 9 means predict into periods that
end at least 9 days after the predictor run. A value of -9 means
predict into periods that end at least 9 days before the predictor run.

Setting these values can give you very flexible control over when
predictions will be made, especially if your predictor job is set to
run daily or more frequently. Before you set these values, you should
think carefully about when you want predictions for a period to start
being made, and when you want them to stop being made. Then you
need to compute the appropriate relative start
and end dates.

例子：

1. **Requirement**: A predictor uses data from the same week
as the predicted value. (No past sampled data are used.)
After the week ends on Sunday, you expect the data
to be entered in the following two days (Monday and Tuesday).
You don't want to start predicting data until Wednesday after the
week ends because you don't want partial results to be shown.
However, data may still be adjusted on Wednesday, so you want to
update the predictions also on Thursday. After that, the
data are frozen and you don't want to predict for that period anymore.

    **Solution:** For a job running daily or more frequently, define the
    relative start as -10 and the relative end as -2 (for periods
    within 10 to 2 days before the job runs).

    - Before Wednesday of the following week, the period end is
    greater than 2 days before, so no predictions are made.

    - On Wednesday of the following week, the period started 9 days
    before and ended 2 days before. Predictions are made because -9 to -2
    are within the range -10 to -2.

    - On Thursday of the following week, the period started 10 days
    before and ended 3 days before. Predictions are made because -10 to -3
    are within the range -10 to -2.

    - After Thursday, the previous week started more than
    10 days before, so no predictions are made.

    - Predictions are made only on Wednesday and Thursday. On Friday through
    Tuesday, no predictions are made (and the job finishes very quickly).

2. **Requirement**: A predictor is used to forecast a limit (average plus twice
the standard deviation)
for expected non-seasonally varying disease cases based on data from the
previous five weeks. Weeks are Monday through Sunday. Predictions should start
being made from the previous Tuesday, using available data at that time, and
continue being made through Tuesday of the week that the predictions are being
made for (by which time it is assumed that the previous week's data are final).

    **Solution:** For a job running daily or more frequently,
    define the relative start as -1 and the relative end as 12.

    - Before Tuesday, predictions will not be made for the following week because it
    ends more than 12 days later.

    - On Tuesday, predictions will be made for the following week which starts
    in 6 days and ends in 12 days.

    - On Wednesday through the following Tuesday, predictions will be made for
    the week whose start-to-end dates are Wed: 5 to 11, Thu: 4 to 10,
    Fri: 3 to 9, Sat: 2 to 8, Sun: 1 to 7, Mon: 0 to 6, and Tue: -1 to 5.

    - Note that on Tuesday, predictions are made for the current week with
    start-to-end dates -1 to 5, and also for the following week
    with start-to-end dates 6 to 12. On all other days of the week
    predictions are made for one week.

You can select which predictors and predictor groups will run during the job:

- **Predictors** runs individual predictors.
They run in the order added.

- **Predictor groups** runs predictor groups.
They run in the order added. The predictors within each group run in the
order of their names (comparing Unicode character values).

If both individual predictors and predictor groups are selected in the same
job, the individual predictors run first, followed by the predictor groups.

