---
revision_date: '2024-04-17'
tags:
- Manage
- DHIS核心 主版
template: single.html
---

# 安装 { #installation } 

安装章节提供了有关如何在以下位置安装DHIS2的信息
各种环境，包括在线中央服务器，离线本地
网络，独立应用程序和称为DHIS2的自包含程序包
生活。

## 介绍 { #install_introduction } 

DHIS2 runs on all platforms for which there exists a Java JDK, which includes most popular operating
systems such as Windows, Linux and Mac. DHIS2 runs on the PostgreSQL
database system. DHIS2 is packaged as a standard Java Web Archive
(WAR-file) and thus runs on any Servlet containers such as Tomcat and
Jetty.

The DHIS2 team recommends Ubuntu 18.04 LTS operating system, PostgreSQL
database system and Tomcat Servlet container as the preferred
environment for server installations.

本章提供了设置上述技术堆栈的指南。
但是，应将其作为起步和运行的指南，而不是
作为上述环境的详尽文档。我们提到
到官方的Ubuntu，PostgreSQL和Tomcat文档进行深入了解
阅读。

The `dhis2-tools` Ubuntu package automates many of the tasks described in
the guide below and is recommended for most users, especially those who
are not familiar with the command line or administration of servers. It
is described in detail in a separate chapter in this guide.

## 服务器规格 { #install_server_specifications } 

DHIS2是数据库密集型应用程序，需要您的服务器
具有适当数量的RAM，CPU核心数量和快速磁盘。
这些建议应被视为经验法则，而不是
确切的措施。 DHIS2在RAM的数量和数量上线性扩展
CPU内核，因此您负担得起的费用越多，应用程序的性能就会越好。

- * RAM：*小型实例至少2 GB，中型实例至少12 GB，大型实例至少64 GB。
- *CPU cores:* 4 CPU cores for a small instance, 8 CPU cores for a medium instance, 16 CPU cores or more for a large instance.
- *磁盘：*建议将SSD用作存储设备。最低要求
  读取速度为150 Mb / s，200 Mb / s好，350 Mb / s或更高
  ideal. At least 100 GB storage space is recommended, but
  将完全取决于其中包含的数据量
  数据值表。 Analytics（分析）表格需要大量
  storage space. Plan ahead and ensure that your server can be upgraded
  根据需要具有更多的磁盘空间。

## 软件需求 { #install_software_requirements } 

更高版本的DHIS2需要以下软件版本才能运行。

- An operating system for which a Java JDK or JRE version 17 exists. Linux is recommended.
- Java JDK. OpenJDK is recommended.  


Table: DHIS2 JDK compatibility

| DHIS2 version | JDK recommended | JDK required |
|---------------|-----------------|--------------|
| 2.41          | 17              | 17           |
| 2.40          | 17              | 11           |
| 2.38          | 11              | 11           |
| 2.35          | 11              | 8            |
| pre 2.35      | 8               | 8            |


- PostgreSQL database version 9.6 or later. A later PostgreSQL version such as version 14 is recommended.
- PostGIS数据库扩展版本2.2或更高版本。
- Tomcat Servlet容器版本8.5.50或更高版本，或其他Servlet API
  3.1兼容的servlet容器。
- 仅群集设置（可选）：Redis数据存储版本4或更高版本。

## 服务器设置 { #install_server_setup } 

This section describes how to set up a server instance of DHIS2 on
Ubuntu 18.04 64 bit with PostgreSQL as database system and Tomcat as
Servlet container. This guide is not meant to be a step-by-step guide
per se, but rather to serve as a reference to how DHIS2 can be deployed
on a server. There are many possible deployment strategies, which will
differ depending on the operating system and database you are using, and
other factors. The term *invoke* refers to executing a given command in
a terminal.

For this guide we assume that 8 Gb RAM is allocated for PostgreSQL and 8
GB RAM is allocated for Tomcat/JVM, and that a 64-bit operating system
is used. *If you are running a different configuration please adjust the
suggested values accordingly\!*

We recommend that the available memory
is split roughly equally between the database and the JVM. Remember to
leave some of the physical memory to the operating system for it to
perform its tasks, for instance around 2 GB. The steps marked as
*optional*, like the step for performance tuning, can be done at a later
stage.

### 创建一个用户来运行DHIS2 { #install_creating_user } 

您应该创建一个专用用户来运行DHIS2。

> **重要**
>
>您不应以root用户等特权用户身份运行DHIS2服务器。

通过调用以下命令创建一个名为dhis的新用户：

```sh
sudo useradd -d / home / dhis -m dhis -s / bin / false
```

然后为您的帐户调用设置密码：

```sh
须藤密码
```

确保设置了一个安全密码，该密码至少包含15个随机字符。

### 创建配置目录 { #install_creating_config_directory } 

首先为DHIS2配置创建合适的目录
文件。此目录还将用于应用程序，文件和日志文件。
示例目录可以是：

```sh
sudo mkdir /home/dhis/config
sudo chown dhis:dhis /home/dhis/config
```

DHIS2 will look for an environment variable called `DHIS2_HOME` to
locate the DHIS2 configuration directory. This directory will be
referred to as `DHIS2_HOME` in this installation guide. We will define
the environment variable in a later step in the installation process.

If no environment variable `DHIS2_HOME` is found, the default 
configuration file location `/opt/dhis2` is used.

### 设置服务器时区和语言环境 { #install_setting_server_tz } 

可能需要重新配置服务器的时区以匹配
DHIS2服务器将覆盖的位置的时区。
如果您使用的是虚拟专用服务器，则默认时区可能不会
对应于您的DHIS2位置的时区。您可以轻松地
通过调用以下内容并按照以下说明重新配置时区
说明。

```sh
sudo dpkg-重新配置tzdata
```

PostgreSQL对语言环境敏感，因此您可能必须安装
地区优先。要检查现有的语言环境并安装新的语言环境（例如
挪威）：

```sh
语言环境-a
须藤locale-gen nb_NO.UTF-8
```

### PostgreSQL安装 { #install_postgresql_installation } 

通过以下方式安装PostgreSQL
    调用：

```sh
sudo apt-get install -y postgresql-12 postgresql-12-postgis-3
```

通过调用以下命令创建一个名为* dhis *的非特权用户：

```sh
须藤-u postgres createuser -SDRP dhis
```

在提示符下输入安全密码。通过调用创建数据库：

```sh
须藤-u postgres createdb -O dhis dhis2
```

通过调用`exit`返回您的会话现在您有一个PostgreSQL用户
称为* dhis *和一个名为* dhis2 *的数据库。

* PostGIS *扩展是多种GIS /映射功能所必需的
工作。 DHIS 2将尝试在安装过程中安装PostGIS扩展
启动。如果DHIS 2数据库用户没有创建权限
您可以使用* postgres *用户从控制台创建扩展
使用以下命令：

```sh
sudo -u postgres psql -c“创建扩展名postgis;” dhis2
```

For adding trigram indexes and compounding it with primitive column types, two extensions have to be created in the database for DHIS 2 verision 2.38 and later. The extensions are already part of the default posgresql installation:

```sh
sudo -u postgres psql -c "create extension btree_gin;" dhis2
sudo -u postgres psql -c "create extension pg_trgm;" dhis2
```

退出控制台，并使用* \\ q *并返回到先前的用户
*出口*。

### PostgreSQL性能调优 { #install_postgresql_performance_tuning } 

Tuning PostgreSQL is required to achieve a high-performing system but
is optional in terms of getting DHIS2 to run. The various settings can be
specified in the `postgresql.conf` configuration file or, preferably, in a specific
file in the `conf.d` directory. The settings is based on allocating 8 GB RAM to
PostgreSQL and should be adjusted accordingly to the environment.

```sh
须藤nano /etc/postgresql/12/main/postgresql.conf
```

Set the following properties.

```properties
jit = off
```

This is important to set for postgresql versions 12 and greater.  The jit compiler 
functionality causes a significant slowdown on many DHIS2 specific queries, eg 
Program Indicator queries.  For versions 11 and below, the setting is off by default.

```属性
max_connections = 200
```

确定PostgreSQL允许的最大连接数。

```properties
shared_buffers = 3GB
```

确定应专门分配多少内存
PostgreSQL缓存。此设置控制共享内核的大小
应该为PostgreSQL保留的内存。应该设置为
PostgreSQL专用内存的40％。

```properties
work_mem = 24MB
```

确定用于内部排序和哈希的内存量
操作。此设置是针对每个连接，针对每个查询的，因此需要大量内存
如果将其提高得太高，可能会被消耗掉。正确设置该值
对于DHIS2聚合性能至关重要。

```properties
maintenance_work_mem = 1GB
```

确定PostgreSQL可用于维护的内存量
创建索引，运行真空，添加外部文件等操作
键。增大此值可能会提高索引创建的性能
在分析生成过程中。

```properties
temp_buffers = 16MB
```

Sets the maximum number of temporary buffers used by each database 
session. These are session-local buffers used only for access to temporary 
tables. 

```properties
effective_cache_size = 8GB
```

An estimate of how much memory is available for disk caching by the
operating system (not an allocation) and is used by PostgreSQL to
determine whether a query plan will fit into memory or not. Setting it
to a higher value than what is really available will result in poor
performance. This value should be inclusive of the `shared_buffers`
setting. PostgreSQL has two layers of caching: The first layer uses the
kernel shared memory and is controlled by the shared\_buffers setting.
PostgreSQL delegates the second layer to the operating system disk cache
and the size of available memory can be given with the
`effective_cache_size` setting.

```属性
checkpoint_completion_target = 0.8
```

设置WAL写过程中用于缓冲的内存。
增大此值可能会提高大量写入系统的吞吐量。

```属性
sync_commit =关
```

指定事务提交是否将等待WAL记录
是否将其写入磁盘，然后再返回客户端。设定这个
关闭将大大提高性能。这也意味着那里
交易之间的轻微延迟被报告为成功
客户端，它实际上是安全的，但是数据库状态不能为
已损坏，这是性能密集型和
像DHIS2这样的重写入系统。

```properties
wal_writer_delay = 10s
```

指定WAL写操作之间的延迟。将此设置为较高
价值可能会提高大量写入系统的性能，因为
一次刷新到磁盘就可以执行许多写操作。

```属性
random_page_cost = 1.1
```

*仅SSD。*设置查询计划程序对非连续获取的磁盘页面的成本的估计。较低的值将导致系统比顺序扫描更喜欢索引扫描。对于在SSD上运行的数据库或在内存中大量缓存的数据库，较低的值有意义。默认值为4.0，这对于传统磁盘而言是合理的。

```属性
max_locks_per_transaction = 96
```

指定为每个事务分配的对象锁的平均数量。设置该参数主要是为了允许完成涉及大量表的升级例程。

```properties
track_activity_query_size = 8192
```

Specifies the number of bytes reserved to track the currently executing command for each active session. Useful to view the full query string for monitoring of currently running queries.

```properties
jit = off
```

This setting turns the jit optimizer off.  It should be set to off for postgresql versions 12 and upwards.  Many queries, particularly program indicator queries, perform very badly with the default enabled jit setting.  Turning it off can improve response times by up to 100x with resulting significant improvement in dashboard performance.

通过调用以下命令来重新启动PostgreSQL：

```sh
sudo systemctl restart postgresql
```

### Java安装 { #install_java_installation } 

The recommended Java JDK for DHIS 2 is OpenJDK 17 (for version 2.40 and later). You can install it with the following command:

```
sudo apt-get install -y openjdk-17-jdk
```

通过调用以下命令来验证安装是否正确：

```
Java版本
```

### DHIS2配置 { #install_database_configuration } 

The database connection information is provided to DHIS2 through a
configuration file called `dhis.conf`. Create this file and save it in
the `DHIS2_HOME` directory. As an example this location could be:

```sh
/home/dhis/config/dhis.conf
```

与上述设置相对应的PostgreSQL配置文件具有
这些属性：

```properties
# ----------------------------------------------------------------------
# Database connection
# ----------------------------------------------------------------------

# JDBC driver class
connection.driver_class = org.postgresql.Driver

# Database connection URL
connection.url = jdbc:postgresql:dhis2

# Database username
connection.username = dhis

# Database password
connection.password = xxxx

# ----------------------------------------------------------------------
# Server
# ----------------------------------------------------------------------

# Enable secure settings if deployed on HTTPS, default 'off', can be 'on'
# server.https = on

# Server base URL
# server.base.url = https://server.com
```

强烈建议启用`server.https`设置并使用加密的HTTPS协议部署DHIS 2。此设置将启用例如安全cookie。启用此设置后，需要进行HTTPS部署。

`server.base.url`设置是指最终用户通过网络访问系统的URL。

Note that the configuration file supports environment variables. This
means that you can set certain properties as environment variables and
have them resolved, e.g. like this where `DB\_PASSWD` is the
name of the environment variable:

```属性
connection.password = $ {DB_PASSWD}
```

请注意，此文件包含您的DHIS2数据库的密码（以明文形式）
文本，因此需要对其进行保护，以防止未经授权的访问。去做这个，
调用以下命令，以确保仅允许* dhis *用户读取它：

```sh
chmod 600 dhis.conf
```

### Tomcat和DHIS2安装 { #install_tomcat_dhis2_installation } 

要安装Tomcat Servlet容器，我们将利用Tomcat用户
通过调用打包：

```sh
sudo apt-get install -y tomcat8-user
```

This package lets us easily create a new Tomcat instance. The instance
will be created in the current directory. An appropriate location is the
home directory of the `dhis` user:

```sh
sudo tomcat8-instance-create /home/dhis/tomcat-dhis
sudo chown -R dhis:dhis /home/dhis/tomcat-dhis/
```

This will create an instance in a directory called `tomcat-dhis`. Note
that the `tomcat8-user` package allows for creating any number of DHIS2
instances if that is desired.

Next edit the file `tomcat-dhis/bin/setenv.sh` and add the lines below.

* `JAVA_HOME` sets the location of the JDK installation.
* `JAVA_OPTS` passes parameters to the JVM.
    * `-Xms` sets the initial allocation of memory to the Java heap memory space.
    * `-Xmx` sets the maximum allocation of memory to the Java heap memory space. This should reflect how much memory you would like to allocate to the DHIS 2 software application on your server.
* `DHIS2_HOME` sets the location of the `dhis.conf` configuration file for DHIS 2.

Check that the path the Java binaries are correct as they might vary from system to system, e.g. on AMD systems you might see
`/java-11-openjdk-amd64`. Note that you should adjust these values to your environment.

```sh
JAVA_HOME='/usr/lib/jvm/java-11-openjdk-amd64/'
JAVA_OPTS='-Xms4000m -Xmx7000m'
DHIS2_HOME='/home/dhis/config'
```

The Tomcat configuration file is located in
`tomcat-dhis/conf/server.xml`. The element which defines the connection
to DHIS is the *Connector* element with port 8080. You can change the
port number in the Connector element to a desired port if necessary. 
The `relaxedQueryChars` attribute is necessary to allow certain characters 
in URLs used by the DHIS2 front-end.

```xml
<Connector port="8080" protocol="HTTP/1.1"
  connectionTimeout="20000"
  redirectPort="8443"
  relaxedQueryChars="[]" />
```

The next step is to download the DHIS2 WAR file and place it into the
_webapps_ directory of Tomcat. You can download DHIS2 WAR files from the following location: 

```sh
https://releases.dhis2.org/
```

Move the WAR file into the Tomcat `webapps` directory. We want to call the
WAR file `ROOT.war` in order to make it available at `localhost` directly
without a context path:

```sh
mv dhis.war tomcat-dhis / webapps / ROOT.war
```

DHIS2 should never be run as a privileged user. After you have modified
the `setenv.sh file`, modify the startup script to check and verify that the
script has not been invoked as root.

```sh
＃！/ bin / sh
设置-e

如果[“ $（id -u）” -eq“ 0”];然后
  回声“此脚本不能以root用户身份运行” 1>＆2
  1号出口
科幻

导出CATALINA_BASE =“ / home / dhis / tomcat-dhis”
/usr/share/tomcat8/bin/startup.sh
回显“ Tomcat启动”
```

### 运行DHIS2 { #install_running_dhis2 } 

DHIS2现在可以通过调用来启动：

    须藤-u dhis tomcat-dhis / bin / startup.sh

> **重要**
>
>绝对不要以root或其他特权用户身份运行DHIS2服务器。

DHIS2可以通过调用来停止：

    须藤-u dhis tomcat-dhis / bin / shutdown.sh

要监视Tomcat的行为，日志是该日志的主要来源
信息。可以使用以下命令查看日志：

    尾巴-f tomcat-dhis / logs / catalina.out

假设WAR文件名为ROOT.war，您现在可以访问
DHIS2实例位于以下URL：

    http://localhost:8080

## 文件存储配置 { #install_file_store_configuration } 

DHIS2 is capable of capturing and storing files. By default, files will be stored on the local file
system of the server which runs DHIS2 in a *files* directory under the `DHIS2_HOME` external
directory location. The directory *files* can be changed via the `filestore.container` property in
the `dhis.conf`.

You can also configure DHIS2 to store files on cloud-based storage providers. AWS S3 or S3
compatible object stores are currently supported.

To enable storage in AWS S3 you must define the following additional properties in your `dhis.conf`
file:

```properties
# File store provider. Currently 'filesystem' (default), 'aws-s3' and 's3' are supported.
filestore.provider = 'aws-s3'

# Directory in external directory on local file system or bucket in AWS S3 or S3 API
filestore.container = files

# The following configuration is applicable to cloud storage only (provider 'aws-s3' or 's3')

# Datacenter location. Optional but recommended for performance reasons.
filestore.location = eu-west-1

# Username / Access key for AWS S3 or S3 APIs
filestore.identity = xxxx

# Password / Secret key for AWS S3 or S3 APIs (sensitive)
filestore.secret = xxxx
```

To enable storage in an S3 compatible object store you must define the following additional
properties in your `dhis.conf` file:

```properties
# File store provider. Currently 'filesystem' (default), 'aws-s3' and 's3' are supported.
filestore.provider = 's3'

# Directory in external directory on local file system or bucket in AWS S3
filestore.container = files

# The following configuration is applicable to cloud storage only (provider 'aws-s3' or 's3')

# URL where the S3 compatible API can be accessed (only for provider 's3')
filestore.endpoint = http://minio:9000 

# Datacenter location. Optional but recommended for performance reasons.
filestore.location = eu-west-1

# Username / Access key for AWS S3 or S3 APIs
filestore.identity = xxxx

# Password / Secret key for AWS S3 or S3 APIs (sensitive)
filestore.secret = xxxx
```

> **注意**
>
>如果您在dhis.conf中配置了云存储，则上传的所有文件
>或系统生成的文件将使用云存储。

These configurations are examples and should be changed to fit your needs.
For a production system the initial setup of the file store should be
carefully considered as moving files across storage providers while
keeping the integrity of the database references could be complex. Keep
in mind that the contents of the file store might contain both sensitive
and integral information and protecting access to the folder as well as
making sure a backup plan is in place is recommended on a production
implementation.

> **Note**
> 
> AWS S3 and S3 compatible object stores are the only supported cloud providers but more providers
> could be added. Let us know if you have a use case for additional providers.

## Google服务帐户配置 { #install_google_service_account_configuration } 

DHIS2 can connect to various Google service APIs. For instance, the
DHIS2 Maps app can utilize the Google Earth Engine API to load Earth Engine map
layers. There are 2 ways to obtain the Google API key.

### Set it up yourself { #set-it-up-yourself } 

Set up a Google service account and create a private key:

  - 创建一个Google服务帐户。请咨询[Google身份
    平台]（https://developers.google.com/identity/protocols/OAuth2ServiceAccount#overview）
    文档。

  - 访问[Google云控制台]（https://console.cloud.google.com）
    并转到API Manager \>凭据\>创建凭据\>
    服务帐户密钥。选择您的服务帐户和JSON作为密钥
    键入并单击创建。

  - 将JSON密钥重命名为* dhis-google-auth.json *。

After downloading the key file, put the `dhis-google-auth.json` file in
the `DHIS2_HOME` directory (the same location as the `dhis.conf` file).
As an example this location could be:

    /home/dhis/config/dhis-google-auth.json

### Send an email to set up the Google Earth Engine API key { #send-an-email-to-set-up-the-google-earth-engine-api-key } 

If you only intend to use the key for the Google Earth Engine map layers,
you can simply send an email. See the [Google Earth Engine API key documentation](https://docs.dhis2.org/en/topics/tutorials/google-earth-engine-sign-up.html).

## Bing Maps API key { #install_bing_maps_api_key }

To enable use of Bing Maps basemap layers, you need to set up the Bing Maps API key. See [Bing Maps API key documentation](https://www.microsoft.com/en-us/maps/bing-maps/create-a-bing-maps-key) for information on setting up the key.

## OpenID Connect（OIDC）配置 { #install_oidc_configuration } 

DHIS2 supports the OpenID Connect (OIDC) identity layer for single sign-in (SSO). OIDC is a standard authentication protocol that lets users sign in with an identity provider (IdP) such as for example Google. After users have successfully signed in to their IdP, they will be automatically signed in to DHIS2.

This section provides general information about using DHIS2 with an OIDC provider, as well as complete configuration examples.

The DHIS2 OIDC 'authorization code' authentication flow:

1. A user attempts to log in to DHIS2 and clicks the OIDC provider button on the login page.

2. DHIS2 redirects the browser to the IdP's login page.

3. If not already logged in, the user is prompted for credentials. When successfully authenticated, the IdP responds with a redirect back to the DHIS2 server. The redirect includes a unique authorization code generated for the user.

4. The DHIS2 server internally sends the user's authorization code back to the IdP server along with its own client id and client secret credentials.

5. The IdP returns an ID token back to the DHIS2 server. DHIS2 server performs validation of the token.

6. The DHIS2 server looks up the internal DHIS2 user with the mapping claims found in the ID token (defaults to email), authorizes the user and completes the login process.

### Requirements for using OIDC with DHIS2: { #requirements-for-using-oidc-with-dhis2 } 

#### IdP server account { #idp-server-account } 

You must have an admin account on an online identity provider (IdP) or on a standalone server that are supported by DHIS2.

The following IdPs are currently supported and tested:

* 谷歌
* Azure AD
* WSO2
* Okta (See separate tutorial: [here](#configure-openid-connect-with-okta))

There is also a **generic provider** config which can support "any" OIDC compatible provider.

#### DHIS2 user account { #dhis2-user-account } 

You must explicitly create the users in the DHIS2 server before they can log in with the identity provider. Importing them from an external directory such as Active Directory is currently not supported. Provisioning and management of users with an external identity store is not supported by the OIDC standard.

#### 用户的IdP声明和映射 { #idp-claims-and-mapping-of-users } 

To sign in to DHIS2 with OIDC, a given user must be provisioned in the IdP and then mapped to a pre created user account in DHIS2. OIDC uses a method that relies on claims to share user account attributes with other applications. Claims include user account attributes such as email, phone number, name, etc. DHIS2 relies on a IdP claim to map user accounts from the IdP to those in the DHIS2 server. By default, DHIS2 expects the IdP to pass the _email_ claim. Depending on your IdP, you may need to configure DHIS2 to use a different IdP claim.

If you are using Google or Azure AD as an IdP, the default behavior is to use the _email_ claim to map IdP identities to DHIS2 user accounts.

> **Note**
>
> In order for a DHIS2 user to be able to log in with an IdP, the user profile checkbox: *External authentication only OpenID or LDAP* must be checked and *OpenID* field must match the claim (mapping claim) returned by the IdP. Email is the default claim used by both Google and Azure AD.

### 为OIDC配置身份提供者 { #configure-the-identity-provider-for-oidc } 

This topic provides general information about configuring an identity provider (IdP) to use OIDC with DHIS2. This is one step in a multi-step process. Each IdP has slightly different ways to configure it. Check your IdP's own documentation for how to create and configure an OIDC application. Here we refer to the DHIS2 server as the OIDC "application".

#### 重定向网址 { #redirect-url } 

All IdPs will require a redirect URL to your DHIS2 server. 
You can construct it using the following pattern:

```
(protocol):/(your DHIS2 host)/oauth2/code/PROVIDER_KEY
```

Example when using Google IdP:

```
https://mydhis2-server.org/oauth2/code/google
```

External links to instructions for configuring your IdP:

* [Google](https://developers.google.com/identity/protocols/oauth2/openid-connect)
* [Azure AD tutorial](https://medium.com/xebia-engineering/authentication-and-authorization-using-azure-active-directory-266980586ab8)


### Example setup for Google { #example-setup-for-google } 

1. Register an account and sign in. For example, for Google, you can go to the Google [developer console](https://console.developers.google.com).
2. In the Google developer dashboard, click 'create a new project'.
3. Follow the instructions for creating an OAuth 2.0 client ID and client secret.
4. Set your "Authorized redirect URL" to: `https://mydhis2-server.org/oauth2/code/google`
5. Copy and keep the "client id" and "client secret" in a secure place.

> **Tip**
>
> When testing on a local DHIS2 instance running for example on your laptop, you can use localhost as the redirect URL, like this: `https://localhost:8080/oauth2/code/google`
> *Remember to also add the redirect URL in the Google developer console*

#### Google dhis.conf example: { #google-dhisconf-example } 
```properties

# Enables OIDC login
oidc.oauth2.login.enabled = on

# Client id, given to you in the Google developer console
oidc.provider.google.client_id = my client id

# Client secret, given to you in the Google developer console
oidc.provider.google.client_secret = my client secret

# [Optional] Authorized redirect URI, the same as set in the Google developer console 
# If your public hostname is different from what the server sees internally, 
# you need to provide your full public url, like the example below.
oidc.provider.google.redirect_url = https://mydhis2-server.org/oauth2/code/google

# [Optional] Where to redirect after logging out.
# If your public hostname is different from what the server sees internally, 
# you need to provide your full public url, like the example below. 
oidc.logout.redirect_url = https://mydhis2-server.org

```

### Example setup for Azure AD { #example-setup-for-azure-ad } 

Make sure your Azure AD account in the Azure portal is configured with a redirect URL like: `(protocol):/(host)/oauth2/code/PROVIDER_KEY`. 
To register your DHIS2 server as an "application" in the Azure portal, follow these steps:

> **Note**
>
> PROVIDER_KEY is the "name" part of the configuration key, example: "oidc.provider.PROVIDER_KEY.tenant = My Azure SSO"
> If you have multiple Azure providers you want to configure, you can use this name form: (azure.0), (azure.1) etc.
> Redirect URL example: https://mydhis2-server.org/oauth2/code/azure.0

1. 搜索并选择*应用程序注册*。
2. 点击*新注册*。
3. In the *Name* field, enter a descriptive name for your DHIS2 instance.
4. In the *Redirect URI* field, enter the redirect URL as specified above.
5. 点击*注册*。

#### Azure AD dhis.conf example: { #azure-ad-dhisconf-example } 
```properties

# Enables OIDC login
oidc.oauth2.login.enabled = on

# First provider (azure.0):

# Alias, or name that will show on the login button in the DHIS2 login screen.
oidc.provider.azure.0.tenant = organization name

# Client id, given to you in the Azure portal
oidc.provider.azure.0.client_id = my client id

# Client secret, given to you in the Azure portal
oidc.provider.azure.0.client_secret = my client secret

# [Optional] Authorized redirect URI, the as set in Azure portal 
# If your public hostname is different from what the server sees internally, 
# you need to provide your full public url, like the example below.
oidc.provider.azure.0.redirect_url = https://mydhis2-server.org/oauth2/code/azure.0

# [Optional] Where to redirect after logging out.
# If your public hostname is different from what the server sees internally, 
# you need to provide your full public URL, like the example below.
oidc.logout.redirect_url = https://mydhis2-server.org

# [Optional], defaults to 'email'
oidc.provider.azure.0.mapping_claim = email

# [Optional], defaults to 'on'
oidc.provider.azure.0.support_logout = on


# Second provider (azure.1):

oidc.provider.azure.1.tenant = other organization name
...
```

### Generic providers { #generic-providers } 

The generic provider can be used to configure "any" standard OIDC provider which are compatible with "Spring Security".

在下面的示例中，我们将使用提供商密钥`helseid`配置挪威政府 _HelseID_ OIDC 提供商。

The defined provider will appear as a button on the login page with the provider key as the default name, 
or the value of the `display_alias` if defined. The provider key is arbitrary and can be any alphanumeric string, 
except for the reserved names used by the specific providers (`google`, `azure.0,azure.1...`, `wso2`).

> **Note**
> The generic provider uses the following hardcoded configuration defaults:
> **(These are not possible to change)**
> * Client Authentication, `ClientAuthenticationMethod.BASIC`: [rfc](https://tools.ietf.org/html/rfc6749#section-2.3)
> * Authenticated Requests, `AuthenticationMethod.HEADER`: [rfc](https://tools.ietf.org/html/rfc6750#section-2) 

#### Generic (helseid) dhis.conf example: { #generic-helseid-dhisconf-example } 

```properties

# Enables OIDC login
oidc.oauth2.login.enabled = on

# Required variables:
oidc.provider.helseid.client_id = CLIENT_ID
oidc.provider.helseid.client_secret = CLIENT_SECRET
oidc.provider.helseid.mapping_claim = helseid://claims/identity/email
oidc.provider.helseid.authorization_uri = https://helseid.no/connect/authorize
oidc.provider.helseid.token_uri = https://helseid.no/connect/token
oidc.provider.helseid.user_info_uri = https://helseid.no/connect/userinfo
oidc.provider.helseid.jwk_uri = https://helseid.no/.well-known/openid-configuration/jwks
oidc.provider.helseid.end_session_endpoint = https://helseid.no/connect/endsession
oidc.provider.helseid.scopes = helseid://scopes/identity/email

# [Optional] Authorized redirect URI, the as set in Azure portal 
# If your public hostname is different from what the server sees internally, 
# you need to provide your full public url, like the example below.
oidc.provider.helseid.redirect_url = https://mydhis2-server.org/oauth2/code/helseid

# [Optional], defaults to 'on'
oidc.provider.helseid.enable_logout = on

# [Optional] Where to redirect after logging out.
# If your public hostname is different from what the server sees internally, 
# you need to provide your full public URL, like the example below.
oidc.logout.redirect_url = https://mydhis2-server.org

# [Optional] PKCE support, see: https://oauth.net/2/pkce/), default is 'false'
oidc.provider.helseid.enable_pkce = on

# [Optional] Extra variables appended to the request. 
# Must be key/value pairs like: "KEY1 VALUE1,KEY2 VALUE2,..."
oidc.provider.helseid.extra_request_parameters = acr_values lvl4,other_key value2

# [Optional] This is the alias/name displayed on the login button in the DHIS2 login page
oidc.provider.helseid.display_alias = HelseID

# [Optional] Link to an url for a logo. (Can use absolute or relative URLs)
oidc.provider.helseid.logo_image = ../security/btn_helseid.svg
# [Optional] CSS padding for the logo image
oidc.provider.helseid.logo_image_padding = 0px 1px
```

### JWT bearer token authentication { #jwt-bearer-token-authentication } 

Authentication with *JWT bearer tokens* can be enabled for clients which API-based when OIDC is configured. 
The DHIS2 Android client is such a type of client and have to use JWT authentication if OIDC login is enabled.

> **Note**
>
> DHIS2 currently only supports the OAuth2 authorization code grant flow for authentication with JWT, (also known as "three-legged OAuth")
> DHIS2 currently only supports using Google as an OIDC provider when using JWT tokens


### 要求 { #requirements } 
* Configure your Google OIDC provider as described above 
* Disable the config parameter ```oauth2.authorization.server.enabled``` by setting it to 'off'
* Enable the config parameter ```oidc.jwt.token.authentication.enabled``` by setting it to 'on'
* Generate an Android OAuth2 client_id as described [here](https://developers.google.com/identity/protocols/oauth2/native-app#creatingcred)

### JWT authentication example { #jwt-authentication-example } 

以下`dhis.conf`部分显示了如何为基于 API 的客户端启用 JWT 身份验证的示例。

```properties

# Enables OIDC login
oidc.oauth2.login.enabled = on

# Minimum required config variables:
oidc.provider.google.client_id = my_client_id
oidc.provider.google.client_secret = my_client_secret

# Enable JWT support
oauth2.authorization.server.enabled = off
oidc.jwt.token.authentication.enabled = on

# Define client 1 using JWT tokens
oidc.provider.google.ext_client.0.client_id = JWT_CLIENT_ID

# Define client 2 using JWT tokens
oidc.provider.google.ext_client.1.client_id = JWT_CLIENT_ID

```

> **Note**
>
> [Check out our tutorial for setting up Okta as a generic OIDC provider.](../../../topics/tutorials/configure-oidc-with-okta.md)

### Connecting a single identity provider account to multiple DHIS2 accounts { #connecting-a-single-identity-provider-account-to-multiple-dhis2-accounts } 

DHIS2 has the ability to map a single identity provider account to multiple DHIS2 accounts. API calls are available to list the linked accounts and also switch between then.

When this option is selected, the `openid` database field in the `userinfo` table does not need to be unique.  When presented with an `openid` value from the identity provider, DHIS2 will log in the user that most recently logged in.

The following `dhis.conf` section shows how to enable linked accounts.

```properties
# Enable a single OIDC account to log in as one of several DHIS2 accounts
linked_accounts.enabled = on
```

For instructions on how to list linked accounts and switch between them, see [*Switching between user accounts connected to the same identity provider account* in the Users chapter of the developer documentation.](../../../develop/using-the-api/dhis-core-version-master/users.html#switching-between-user-accounts-connected-to-the-same-identity-provider-account)

## LDAP配置 { #install_ldap_configuration } 

DHIS2能够使用LDAP服务器进行用户身份验证。
对于LDAP身份验证，要求在
每个LDAP条目的DHIS2数据库。 DHIS2用户将用于代表
权限/用户角色。

To set up LDAP authentication you need to configure the LDAP server URL,
a manager user and an LDAP search base and search filter. This
configuration should be done in the DHIS 2 configuration file `dhis.conf`. 
LDAP users, or entries, are identified by distinguished names 
(DN from now on). An example configuration looks like this:

```属性
＃LDAP服务器网址
ldap.url = ldaps：//domain.org：636

＃LDAP管理器条目专有名称
ldap.manager.dn = cn = johndoe，dc = domain，dc = org

＃LDAP管理员输入密码
ldap.manager.password = xxxx

＃LDAP基本搜索
ldap.search.base = dc = domain，dc = org

＃LDAP搜索过滤器
ldap.search.filter =（cn = {0}）
```

LDAP配置属性说明如下：

- * ldap.url：*要对其进行身份验证的LDAP服务器的URL
  反对。强烈建议使用SSL /加密，以便
  确保身份验证的安全性。例如，URL是
  * ldaps：//domain.org：636 *，其中ldaps是指协议，
  * domain.org *是指域名或IP地址，* 636 *
  指端口（LDAPS默认为636）。
- * ldap.manager.dn：*绑定到LDAP管理器用户是必需的
  用于用户身份验证过程的LDAP服务器。这个性质
  指该条目的DN。即这不是将
  登录DHIS2时需要认证，而不是
  绑定到LDAP服务器以进行身份验证。
- * ldap.manager.password：* LDAP管理器用户的密码。
- * ldap.search.base：*的搜索基础或专有名称
  搜索基础对象，它定义目录中的位置
  LDAP搜索从此开始。
- * ldap.search.filter：*用于匹配条目中DN的过滤器
  LDAP目录。 {0}变量将由DHIS2替换
  用户名，或者为用户定义的LDAP标识符
  使用提供的用户名。

DHIS2将使用提供的用户名/密码并尝试进行身份验证
针对LDAP服务器条目，然后从中查找用户角色/权限
相应的DHIS2用户。这意味着用户必须具有
LDAP目录中的匹配条目以及DHIS2用户，以便
登录。

在身份验证期间，DHIS2将尝试使用以下方式绑定到LDAP服务器：
配置的LDAP服务器URL以及管理员DN和密码。一旦
绑定完成后，它将使用以下命令在目录中搜索条目
配置的LDAP搜索库和搜索过滤器。

配置的过滤器中的{0}变量将在替换之前
应用过滤器。默认情况下，它将被提供的
用户名。您还可以在相关的
DHIS2用户帐户。可以通过DHIS2用户模块用户来完成
通过设置“ LDAP标识符”，在添加或编辑屏幕中找到界面
属性。设置后，LDAP标识符将替换为{0}
过滤器中的变量。 LDAP通用名称时，此功能很有用
不适合或由于某种原因不能用作DHIS2用户名。

## 加密配置 { #install_encryption_configuration } 

DHIS2 allows for encryption of data. Enabling it requires some extra
setup. To provide security to the encryption algorithm you will have to set a
password (key) in the `dhis.conf` configuration file through the
*encryption.password* property:

```属性
加密密码= xxxx
```

The *encryption.password* property is the password (key) used when encrypting
and decrypting data in the database.

If an encryption password is not defined in `dhis.conf`, a default password will be
used. Note that using the default password does not offer any added security due to 
the open source nature of DHIS 2.

Note that the password must not be changed once it has been set and data has been encrypted, as the data can then no longer be decrypted by the application.

密码必须至少为** 24个字符长**。混合数字
建议使用大小写字母。加密密码
必须保密。

> **重要**
>
>如果丢失或更改了加密密码，则无法恢复加密的数据。如果密码丢失，则加密的数据也会丢失。相反，如果
>密码已泄露。因此，应考虑将密码存储在安全的地方。

Note that since the encryption key is stored in the `dhis.conf` configuration file and not
within the database, when moving a database between server environments thorugh a dump and restore, the encryption key must be the same across environments to allow DHIS 2 to
decrypt database content.

Note that encryption support depends on the *Java Cryptography Extension* (JCE) policy files to be available. These are included in all versions of OpenJDK and Oracle JDK 8 Update 144 or later.

## 读取副本数据库配置 { #install_read_replica_configuration } 

DHIS 2允许利用主数据库的只读副本
（主DHIS 2数据库）。只读副本的目的是为了增强
数据库读取查询的性能并扩展容量
超越了单个数据库的限制。大量读取操作，例如
因为分析和事件查询将从中受益。

The configuration requires that you have created one or more replicated
instances of the master DHIS 2 database. PostgreSQL achieves this
through a concept referred to as *streaming replication*. Configuring
read replicas for PostgreSQL is not covered in this guide.

Read replicas can be defined in the `dhis.conf` configuration file. You
can specify up to 5 read replicas per DHIS 2 instance. Each read replica
is denoted with a number between 1 and 5. The JDBC connection URL must
be defined per replica. The username and password can be specified; if
not, the username and password for the master database will be used
instead.

The configuration for read replicas in `dhis.conf` looks like the below.
Each replica is specified with the configuration key *readN* prefix,
where N refers to the replica number.

```属性
＃读取副本1的配置

＃数据库连接URL，用户名和密码
read1.connection.url = jdbc：postgresql：//127.0.0.11/dbread1
read1.connection.username = dhis
read1.connection.password = xxxx

＃读取副本2的配置

＃数据库连接URL，用户名和密码
read2.connection.url = jdbc：postgresql：//127.0.0.12/dbread2
read2.connection.username = dhis
read2.connection.password = xxxx

＃读取副本3的配置

＃数据库连接URL，后退到主用户名和密码
read3.connection.url = jdbc：postgresql：//127.0.0.13/dbread3
```

请注意，您必须重新启动servlet容器才能更改
生效。 DHIS 2将自动在
读取副本。副本的顺序没有任何意义。

## Web服务器群集配置 { #install_web_server_cluster_configuration } 

本节介绍如何设置DHIS 2应用程序以在
簇。

### 群集概述 { #install_cluster_configuration_introduction } 

Clustering is a common technique for improving system scalability and
availability. Clustering refers to setting up multiple web servers such
as Tomcat instances and have them serve a single application. Clustering
allows for *scaling out* an application in the sense that new servers
can be added to improve performance. It also allows for *high
availability* as the system can tolerate instances going down without
making the system inaccessible to users.

有一些方面需要配置才能运行DHIS 2
在集群中。

* 必须安装Redis数据存储，并且必须提供连接信息
be provided for each DHIS 2 application instance in`dhis.conf`.

* DHIS 2 instances and servers must share the same *files* folder used for 
应用程序和文件上传，通过* AWS S3云文件存储*选项
或共享的网络驱动器。

* DHIS 2 instance cache invalidation must be enabled.

* A load balancer such as nginx should be configured to distribute Web requests
跨集群实例。

### DHIS 2 instance cache invalidation with Redis { #install_cluster_cache_invalidation_redis }

DHIS 2 can invalidate the various instance's caches by listening for events sent and emitted from a Redis server, when configured to do so.

This is considered the easiest and preferred way to enable cache invalidation, if you already plan to use [Redis for
shared data store cluster configuration](#install_cluster_configuration_redis), it will share this Redis server for both purposes.

#### Prerequisites { #prerequisites } 

* Redis server

#### Redis configuration { #redis-configuration } 

No specific configuration in Redis is needed for DHIS 2 cache invalidation to work.

When you chose to enable shared data store cluster configuration with Redis, you will share the Redis host/port
configuration with the cache invalidation system. In other words you can only have **one** shared Redis server configured.

#### DHIS 2 configuration { #dhis-2-configuration } 

The following properties must be specified in the DHIS 2 `dhis.conf` configuration file:

```properties
# Cache invalidation config

redis.cache.invalidation.enabled = on

# Shared Redis configuration
redis.host = REDIS_HOST
redis.port = REDIS_PORT
redis.password = PASSWORD (Optional, only if enabled on Redis server)
redis.use.ssl = true (Optional, only if enabled on Redis server) 
```

### Redis共享数据存储集群配置 { #install_cluster_configuration_redis } 

In a cluster setup, a Redis server is required and will handle
shared user sessions, application cache and cluster node leadership.

For optimum performance, *Redis Keyspace events* for _generic commands_ 
and _expired events_ need to be enabled in the Redis Server. If you are 
using a cloud platform-managed Redis server (like *AWS ElastiCache for Redis* 
or *Azure Cache for Redis*), you will have to enable keyspace event notifications 
using the respective cloud console interfaces. If you are setting up a standalone 
Redis server, enabling keyspace event notifications can be done in the 
*redis.conf* file by adding or uncommenting the following line:

```
notify-keyspace-events Egx
```

DHIS2 will connect to Redis if the *redis.enabled* configuration
property in `dhis.conf` is set to *on* along with the following properties:

- * redis.host *：指定Redis服务器在何处运行。默认为* localhost *。必选

- * redis.port *：指定Redis服务器正在侦听的端口。默认为* 6379 *。可选的。

- * redis.password *：指定身份验证密码。如果不需要密码，可以将其留空。

- * redis.use.ssl *：指定Redis服务器是否启用了SSL。默认为false。可选的。默认为* false *。

When Redis is enabled, DHIS2 will automatically assign one of the
running instances as the leader of the cluster. The leader instance will
be used to execute jobs or scheduled tasks that should be run
exclusively by one instance. Optionally you can configure the
*leader.time.to.live.minutes* property in `dhis.conf` to set up how
frequently the leader election needs to occur. It also gives an
indication of how long it would take for another instance to take over
as the leader after the previous leader has become unavailable. The
default value is 2 minutes. Note that assigning a leader in the cluster
is only done if Redis is enabled. An example snippet of the `dhis.conf`
configuration file with Redis enabled and leader election time
configured is shown below.

```properties
# Redis Configuration

redis.enabled = on

# Shared Redis configuration
redis.host = REDIS_HOST
redis.port = REDIS_PORT
redis.password = PASSWORD (Optional, only if enabled on Redis server)
redis.use.ssl = true (Optional, only if enabled on Redis server)

# Optional, defaults to 2 minutes
leader.time.to.live.minutes=4 
```

### 文件文件夹配置 { #files-folder-configuration } 

DHIS 2将在应用程序本身之外存储几种类型的文件，
例如应用程序，保存在数据输入中的文件和用户头像。部署时
在群集中，这些文件的位置必须在所有实例之间共享。
在本地文件系统上，位置为：

```
{DHIS2_HOME} /文件
```

Here, `DHIS2_HOME` refers to the location of the DHIS 2 configuration file
as specified by the DHIS 2 environment variable, and `files` is the file
folder immediately below.

有两种方法可以实现共享位置：

* 使用* AWS S3云文件存储*选项。文件将存储在
S3存储桶，由群集中的所有DHIS 2实例自动共享。
请参阅*文件存储配置*部分以获取指导。
* 设置一个在所有DHIS 2实例之间共享的共享文件夹，并且
集群中的服务器。在Linux上，可以使用* NFS *（网络文件系统）来实现
这是一个分布式文件系统协议。注意只有`files`
subfolder under `DHIS2_HOME` should be shared, not the parent folder. 

### 负载均衡器配置 { #install_load_balancing } 

设置了Tomcat实例集群，这是路由的常用方法
传入Web请求到参与
集群正在使用*负载均衡器*。负载均衡器将确保
负载在群集实例之间平均分配。它也会
检测实例是否不可用，如果是，则停止例程
对该实例的请求，而是使用其他可用实例。

负载平衡可以通过多种方式实现。一个简单的方法是
使用* nginx *，在这种情况下，您将定义一个* upstream *元素，
枚举后端实例的位置，以后再使用
* proxy *位置块中的元素。

```text
http {

  # Upstream element with sticky sessions

  upstream dhis_cluster {
    ip_hash;
    server 193.157.199.131:8080;
    server 193.157.199.132:8080;
  }

  # Proxy pass to backend servers in cluster

  server {
    listen 80;

    location / {
      proxy_pass   http://dhis_cluster/;
    }
  }
}
```

DHIS 2在一定程度上将用户会话的服务器端状态保持不变。
使用“粘性会话”是避免复制
服务器会话状态，方法是将请求从同一客户端路由到
同一台服务器。上游元素中的* ip \ _hash *指令可确保
这个。

请注意，为简洁起见，已省略了几条说明
上面的例子。请查阅反向代理部分以获取详细指南。

## ActiveMQ Artemis configuration { #webapi_artemis_configuration } 

By default DHIS2 will start an embedded instance of ActiveMQ Artemis when booting up. For most use-cases, you do not need to do anything. If you have an existing ActiveMQ Artemis service you want to use instead of the embedded instance you can change the default configuration in your `dhis.conf` file with the configuration properties in the following table.

| Property                  | Value (default first) | 描述                                                  |
| ------------------------- | --------------------- | ------------------------------------------------------------ |
| artemis.mode                 | EMBEDDED \| NATIVE    | 默认的`EMBEDDED`模式会在 DHIS2 实例启动时启动内部 AMQP 服务。如果要连接到外部 AMQP 服务，请将模式设置为`NATIVE`。 |
| artemis.host                 | 127.0.0.1             | Host to bind to.                                             |
| artemis.port                 | 15672                 | 如果 mode 为`EMBEDDED`，则嵌入式服务器将绑定到此端口。如果模式为`NATIVE`，客户端将使用此端口进行连接。 |
| artemis.username             | guest                 | 使用`NATIVE`模式时要连接的用户名。               |
| artemis.password             | guest                 | 如果使用`NATIVE`模式连接到的密码。               |
| artemis.embedded.persistence | off \| on         | 如果 mode 为`EMBEDDED`，则此属性控制内部队列的持久性。 |


## 监控 { #monitoring } 

DHIS 2可以导出Prometheus兼容的度量标准以监视DHIS2实例。 DHIS2监视基础结构旨在公开与应用程序运行时相关的指标以及其他与应用程序相关的信息。

与基础架构相关的指标（例如主机指标，Tomcat或Postgres）不会直接由应用程序监视引擎公开，因此必须分别收集它们。该应用程序当前公开的指标是：

- DHIS 2 API（响应时间，调用次数等）
- JVM（堆大小，垃圾回收等）
- 休眠（查询，缓存等）
- C3P0数据库池
- 应用正常运行时间
- 中央处理器

可以使用以下属性在`dhis.conf`中启用监视（所有属性默认为`off`）：

```属性
monitoring.api.enabled =开
monitoring.jvm.enabled =开
monitoring.dbpool.enabled =开
monitoring.hibernate.enabled =关
monitoring.uptime.enabled =开
monitoring.cpu.enabled =开
```

推荐使用Prometheus和Grafana收集和可视化这些指标的方法。

For more information, see the [monitoring infrastructure](https://github.com/dhis2/wow-backend/blob/master/guides/monitoring.md) page and the [Prometheus and Grafana install](#monitoring) chapter.

## 系统配置 { #install_system_configuration } 

This section covers various system configuration properties.

```properties
system.read_only_mode = on | off
```

将系统设置为只读模式。当您在只读副本数据库上运行 DHIS 2 时，这很有用，以避免 DHIS 2 执行数据库写入操作。可以是`开`或`关`。默认为`关闭`。

```properties
system.session.timeout = (seconds)
```

Sets the user session timeout in seconds. Default is 3600 seconds (1 hour).

```properties
system.sql_view_table_protection = on | off
```

启用或禁用 SQL 视图的敏感数据库表保护。这将禁止通过 SQL 视图查询包含敏感数据的数据库表。不建议禁用。可以是`开`或`关`。默认为`开`。

```properties
system.system.sql_view_write_enabled = on | off
```

Enables or disables write permissions for SQL views. This will prohibit SQL view performing underlying writes (query can be a select which requires write permission). Enabling is not recommended. Can be `on` or `off`. Default is `off`.

```properties
system.program_rule.server_execution = on | off
```

启用或禁用服务器端项目规则的执行。这是指具有分配值、发送消息或安排要发送的消息的操作的项目规则。可以是`开`或`关`。默认为`开`。

```properties
system.remote_servers_allowed = https://server1.org/,https://server2.org/
```

Sets the allowed list of servers to be called in relation to the [metadata pull](../developer/web-api/synchronization.md#webapi_sync_metadata_pull) functionality. It accepts comma-separated values, and it's recommended that each server end with a `/` for enhanced security. Default value is empty.


## 反向代理配置 { #install_reverse_proxy_configuration } 

反向代理是代表服务器运行的代理服务器。使用
反向代理与Servlet容器结合使用是可选的，但
有很多优点：

  - 可以将请求映射并传递到多个servlet容器。
    这提高了灵活性，并使其更易于运行
    同一台服务器上的DHIS2实例。这也使得
    在不影响客户端的情况下更改内部服务器设置。

  - DHIS2应用程序可以作为非root用户在端口上运行
    不同于80，这减少了会话的后果
    劫持。

  - 反向代理可以充当单个SSL服务器并进行配置
    检查恶意内容请求，日志请求和
    响应并提供不敏感的错误消息，这将
    提高安全性。

### 基本的Nginx设置 { #install_basic_nginx_setup } 

由于以下原因，我们建议使用[nginx]（http://www.nginx.org）作为反向代理
其低内存占用和易用性。要安装，请调用
以下：

    sudo apt-get install -y nginx

现在可以使用以下命令启动，重新加载和停止nginx
命令：

    sudo /etc/init.d/nginx开始
    须藤/etc/init.d/nginx重新加载
    sudo /etc/init.d/nginx停止

Now that we have installed nginx we will now continue to configure
regular proxying of requests to our Tomcat instance, which we assume
runs at `http://localhost:8080`. To configure nginx you can open the
configuration file by invoking:

    须藤nano /etc/nginx/nginx.conf

nginx配置围绕代表以下内容的块层次结构构建
http，服务器和位置，其中每个块都从父级继承设置
块。以下代码段将nginx配置为通过代理
（重定向）来自端口80的请求（该端口是nginx监听的端口
默认情况下）到我们的Tomcat实例。包括以下配置
在nginx.conf中：

```text
http {
  gzip on; # Enables compression, incl Web API content-types
  gzip_types
    "application/json;charset=utf-8" application/json
    "application/javascript;charset=utf-8" application/javascript text/javascript
    "application/xml;charset=utf-8" application/xml text/xml
    "text/css;charset=utf-8" text/css
    "text/plain;charset=utf-8" text/plain;

  server {
    listen               80;
    client_max_body_size 10M;

    # Proxy pass to servlet container

    location / {
      proxy_pass                http://localhost:8080/;
      proxy_redirect            off;
      proxy_set_header          Host               $host;
      proxy_set_header          X-Real-IP          $remote_addr;
      proxy_set_header          X-Forwarded-For    $proxy_add_x_forwarded_for;
      proxy_set_header          X-Forwarded-Proto  http;
      proxy_buffer_size         128k;
      proxy_buffers             8 128k;
      proxy_busy_buffers_size   256k;
      proxy_cookie_path         ~*^/(.*) "/$1; SameSite=Lax";
    }
  }
}
```

现在，您可以通过* http：// localhost *访问DHIS2实例。自从
已经设置了反向代理，我们可以通过使Tomcat来提高安全性
只监听本地连接。在* / conf / server.xml *中，您可以添加一个
连接器元素的* address *属性值为* localhost *
对于HTTP 1.1像这样：

```xml
<Connector address="localhost" protocol="HTTP/1.1" />
```

### 使用nginx {#install_enabling_ssl_on_nginx}启用SSL { #install_enabling_ssl_on_nginx } 

为了提高安全性，建议配置服务器
运行DHIS2以通过加密连接与客户端进行通信
并使用受信任的证书向客户端标识自己。这个可以
通过SSL（一种加密通信协议）来实现
在TCP / IP上运行。首先，安装所需的* openssl *库：

    sudo apt-get install -y openssl

To configure nginx to use SSL you will need a proper SSL certificate
from an SSL provider. The cost of a certificate varies a lot depending
on encryption strength. An affordable certificate from [Rapid SSL
Online](http://www.rapidsslonline.com) should serve most purposes. To
generate the CSR (certificate signing request) you can invoke the
command below. When you are prompted for the *Common Name*, enter the
fully qualified domain name for the site you are
    securing.

    openssl req -new -newkey rsa：2048 -nodes -keyout server.key -out server.csr

收到证书文件（.pem或.crt）后，您将
需要将其与生成的server.key文件放在一起
nginx可以到达的位置。一个好的位置可以是
与您的nginx.conf文件所在的目录相同。

Below is an nginx server block where the certificate files are named
server.crt and server.key. Since SSL connections usually occur on port
443 (HTTPS) we pass requests on that port (443) on to the DHIS2 instance
running on `http://localhost:8080`. The first server block will rewrite
all requests connecting to port 80 and force the use of HTTPS/SSL. This
is also necessary because DHIS2 is using a lot of redirects internally
which must be passed on to use HTTPS. Remember to replace
*\<server-ip\>* with the IP of your server. These blocks should replace
the one from the previous section.

```text
http {
  gzip on; # Enables compression, incl Web API content-types
  gzip_types
    "application/json;charset=utf-8" application/json
    "application/javascript;charset=utf-8" application/javascript text/javascript
    "application/xml;charset=utf-8" application/xml text/xml
    "text/css;charset=utf-8" text/css
    "text/plain;charset=utf-8" text/plain;

  # HTTP server - rewrite to force use of SSL

  server {
    listen     80;
    rewrite    ^ https://<server-url>$request_uri? permanent;
  }

  # HTTPS server

  server {
    listen               443 ssl;
    client_max_body_size 10M;

    ssl                  on;
    ssl_certificate      server.crt;
    ssl_certificate_key  server.key;

    ssl_session_cache    shared:SSL:20m;
    ssl_session_timeout  10m;

    ssl_protocols              TLSv1 TLSv1.1 TLSv1.2;
    ssl_ciphers                RC4:HIGH:!aNULL:!MD5;
    ssl_prefer_server_ciphers  on;

    # Proxy pass to servlet container

    location / {
      proxy_pass                http://localhost:8080/;
      proxy_redirect            off;
      proxy_set_header          Host               $host;
      proxy_set_header          X-Real-IP          $remote_addr;
      proxy_set_header          X-Forwarded-For    $proxy_add_x_forwarded_for;
      proxy_set_header          X-Forwarded-Proto  https;
      proxy_buffer_size         128k;
      proxy_buffers             8 128k;
      proxy_busy_buffers_size   256k;
      proxy_cookie_path         ~*^/(.*) "/$1; SameSite=Lax";
    }
  }
}
```

请注意最后一个`https`标头值，该值是通知
servlet容器，该请求通过HTTPS发出。为了
Tomcat还需要使用HTTPS正确生成`Location` URL头
在Tomcat`server.xml`文件中向连接器添加其他两个参数：

```xml
<Connector scheme="https" proxyPort="443" />
```

### 使用Nginx启用缓存 { #install_enabling_caching_ssl_nginx } 

要求提供报告，图表，地图和其他与分析相关的资源
通常会花费一些时间来响应，并且可能会占用大量服务器
资源。为了缩短响应时间，请减少
服务器并隐藏潜在的服务器停机时间，我们可以引入缓存代理
在我们的服务器设置中。缓存的内容将存储在目录中
/ var / cache / nginx，最多将分配250 MB的存储空间。 Nginx的
将自动创建此目录。

```text
http {
  ..
  proxy_cache_path  /var/cache/nginx  levels=1:2  keys_zone=dhis:250m  inactive=1d;


  server {
    ..

    # Proxy pass to servlet container and potentially cache response

    location / {
      proxy_pass                http://localhost:8080/;
      proxy_redirect            off;
      proxy_set_header          Host               $host;
      proxy_set_header          X-Real-IP          $remote_addr;
      proxy_set_header          X-Forwarded-For    $proxy_add_x_forwarded_for;
      proxy_set_header          X-Forwarded-Proto  https;
      proxy_buffer_size         128k;
      proxy_buffers             8 128k;
      proxy_busy_buffers_size   256k;
      proxy_cookie_path         ~*^/(.*) "/$1; SameSite=Lax";
      proxy_cache               dhis;
    }
  }
}
```

> **重要**
>
>请注意，服务器端缓存会缩短DHIS2安全性
>从某种意义上说是功能，这些请求命中了服务器端缓存
>将直接从DHIS2控制范围之外的缓存中提供
>和servlet容器。这意味着请求URL可以是
>猜测并由未经授权的用户从缓存中检索报告。
>因此，如果您捕获敏感信息，请设置服务器端
>不建议使用缓存。

### 使用Nginx进行速率限制 { #install_rate_limiting } 

DHIS 2中的某些Web API调用,如`analytics` API,是计算密集型的。因此，最好对这些API进行速率限制，以允许系统的所有用户充分利用服务器资源。速率限制可以通过`nginx`实现。有多种实现速率限制的方法，这旨在记录基于nginx的方法。

The below nginx configuration will rate limit the `analytics` web API, and has the following elements at the *http* and *location* block level (the configuration is shortened for brevity):

```text
http {
  ..
  limit_req_zone $binary_remote_addr zone=limit_analytics:10m rate=5r/s;

  server {
    ..

    location ~ ^/api/(\d+/)?analytics(.*)$ {
      limit_req    zone=limit_analytics burst=20;
      proxy_pass   http://localhost:8080/api/$1analytics$2$is_args$args;
      ..
    }
  }
}
```

配置的各个元素可以描述为：

- * limit_req_zone $ binary_remote_addr *：速率限制是针对每个请求IP进行的。
- * zone = limit_analytics：20m *：Analytics API的速率限制区域，最多可容纳10 MB的请求IP地址。
- * rate = 20r / s *：每个IP每秒被授予5个请求。
- *location ~ ^/api/(\d+/)?analytics(.\*)$*: Requests for the analytics API endpoint are rate limited.
- *burst=20*: Bursts of up to 20 requests will be queued and serviced at a later point; additional requests will lead to a `503`.

有关完整说明，请查阅[nginx文档]（https://www.nginx.com/blog/rate-limiting-nginx/）。

### 使用Nginx使资源可用 { #install_making_resources_available_with_nginx } 

在某些情况下，希望公开发布某些资源
无需身份验证即可在Web上使用。一个例子是
当您想在Web API中进行与数据分析相关的资源时
在Web门户中可用。以下示例将允许访问
基本的图表，地图，报告，报告表和文档资源
通过将* Authorization * HTTP标头注入
请求。它将从请求中删除Cookie标头，
从响应中获取Set-Cookie标头，以避免更改
当前登录的用户。建议为此创建一个用户
目的仅给出所需的最低权限。授权
值可以通过Base64编码，并在用户名后附加一个
冒号和密码，并以“ Basic”作为前缀，更准确地说是“ Basic”
base64 \ _encode（username：password）“。它将检查使用的HTTP方法
用于请求并返回* 405方法不允许*（如果不是GET，则为其他方法）
检测到。

为此类公共用户设置一个单独的域可能是有利的
使用这种方法时。这是因为我们不想更改
已登录用户访问公共帐户时的凭据
资源。例如，当您的服务器部署在somedomain.com上时，
您可以在api.somedomain.com上设置专用的子域，并指向URL
从您的门户到此子域。

```text
http {
  ..

  server {
    listen       80;
    server_name  api.somedomain.com;

    location ~ ^/(api/(charts|chartValues|reports|reportTables|documents|maps|organisationUnits)|dhis-web-commons/javascripts|images|dhis-web-commons-ajax-json|dhis-web-mapping|dhis-web-visualizer) {
    if ($request_method != GET) {
        return 405;
      }

      proxy_pass         http://localhost:8080;
      proxy_redirect     off;
      proxy_set_header   Host               $host;
      proxy_set_header   X-Real-IP          $remote_addr;
      proxy_set_header   X-Forwarded-For    $proxy_add_x_forwarded_for;
      proxy_set_header   X-Forwarded-Proto  http;
      proxy_set_header   Authorization      "Basic YWRtaW46ZGlzdHJpY3Q=";
      proxy_set_header   Cookie             "";
      proxy_hide_header  Set-Cookie;
    }
  }
}
```


### Block specific Android App versions with nginx { #install_block_android_versions } 

In some scenarios the system administrator might want to block certain Android clients based on its DHIS2 App version. For example, if the users on the field have not updated their Android App version to a specific one and the system administrator wants to block their access to force an update; or completely the opposite scenario when the system administrator wants to block new versions of the App as they have not been yet tested. This can be easily implemented by using specific *User-Agent* rules in the `nginx` configuration file.

```text
http {

  server {
    listen       80;
    server_name  api.somedomain.com;

    # Block the latest Android App as it has not been tested
    if ( $http_user_agent ~ 'com\.dhis2/1\.2\.1/2\.2\.1/' ) {
        return 403;
    }

    # Block Android 4.4 (API is 19) as all users should have received new tablets
    if ( $http_user_agent ~ 'com\.dhis2/.*/.*/Android_19' ) {
        return 403;
    }
  }
}
```

> **Note**
> For the implementation of the method described above note the following: 
> * Before version 1.1.0 the *User-Agent* string was not being sent.
> * From version 1.1.0 to 1.3.2 the *User-Agent* followed the pattern Dhis2/AppVersion/AppVersion/Android_XX
> * From version 2.0.0 and above the *User-Agent* follows the pattern com.dhis2/SdkVersion/AppVersion/Android_XX
> * Android_XX refers to the Android API level i.e. the Android version as listed [here](https://developer.android.com/studio/releases/platforms).
> * nginx uses [PCRE](http://www.pcre.org/) for Regular Expression matching .

## DHIS2 configuration reference (dhis.conf) { #install_dhis2_configuration_reference } 

The following describes the full set of configuration options for the `dhis.conf` configuration file. The configuration file should be placed in a directory which is pointed to by a `DHIS2_HOME` environment variable.

> **注意**
>
>您不应尝试直接使用此配置文件，而应将其用作可用配置选项的参考。许多属性是可选的。

```properties
# ----------------------------------------------------------------------
# Database connection for PostgreSQL [Mandatory]
# ----------------------------------------------------------------------

# Hibernate SQL dialect
connection.dialect = org.hibernate.dialect.PostgreSQLDialect

# JDBC driver class
connection.driver_class = org.postgresql.Driver

# Database connection URL
connection.url = jdbc:postgresql:dhis2

# Database username
connection.username = dhis

# Database password (sensitive)
connection.password = xxxx

# Max size of connection pool (default: 40)
connection.pool.max_size = 40

# ----------------------------------------------------------------------
# Database connection for PostgreSQL [Optional]
# ----------------------------------------------------------------------

# Minimum number of Connections a pool will maintain at any given time (default: 5).
connection.pool.min_size=5

# Initial size of connection pool (default : 5)
#Number of Connections a pool will try to acquire upon startup. Should be between minPoolSize and maxPoolSize
connection.pool.initial_size=5

#Determines how many connections at a time will try to acquire when the pool is exhausted.
connection.pool.acquire_incr=5

#Seconds a Connection can remain pooled but unused before being discarded. Zero means idle connections never expire. (default: 7200)
connection.pool.max_idle_time=7200

#Number of seconds that Connections in excess of minPoolSize should be permitted to remain idle in the pool before being culled (default: 0)
connection.pool.max_idle_time_excess_con=0

#If this is a number greater than 0, dhis2 will test all idle, pooled but unchecked-out connections, every this number of seconds. (default: 0)
connection.pool.idle.con.test.period=0

#If on, an operation will be performed at every connection checkout to verify that the connection is valid. (default: false)
connection.pool.test.on.checkout=false

#If on, an operation will be performed asynchronously at every connection checkin to verify that the connection is valid. (default: on)
connection.pool.test.on.checkin=on

#Defines the query that will be executed for all connection tests. Ideally this config is not needed as postgresql driver already provides an efficient test query. The config is exposed simply for evaluation, do not use it unless there is a reason to.
connection.pool.preferred.test.query=select 1

#Configure the number of helper threads used by dhis2 for jdbc operations. (default: 3)
connection.pool.num.helper.threads=3

# Database datasource pool type. Supported pool types are: 
#
# * c3p0 (default): For information see https://www.mchange.com/projects/c3p0/
# 
# * hikari: For information see https://github.com/brettwooldridge/HikariCP
#
# * unpooled: Some implementations might want to have more control over the pooling and database cluster architecture 
# (e.g., using PgBouncer as pool manager behind HAProxy for load balancing). In these cases, the internal pool is un-necessary 
# and gets in the way.
db.pool.type=c3p0

# ----------------------------------------------------------------------
# Server [Mandatory]
# ----------------------------------------------------------------------

# Base URL to the DHIS 2 instance
server.base.url = https://play.dhis2.org/dev 

# Enable secure settings if system is deployed on HTTPS, can be 'off', 'on'
server.https = off

# ----------------------------------------------------------------------
# System [Optional]
# ----------------------------------------------------------------------

# System mode for database read operations only, can be 'off', 'on'
system.read_only_mode = off

# Session timeout in seconds, default is 3600
system.session.timeout = 3600

# SQL view protected tables, can be 'on', 'off'
system.sql_view_table_protection = on

# SQL view write enabled, can be 'on', 'off'
system.sql_view_write_enabled = off

# Disable server-side program rule execution, can be 'on', 'off'
system.program_rule.server_execution = on

# Remote servers which the server is allowed to call
# Accepts comma-separated values
# Servers should end with '/' for enhanced security
# Default is empty
system.remote_servers_allowed = https://server1.org/,https://server2.org/

# ----------------------------------------------------------------------
# Encryption [Optional]
# ----------------------------------------------------------------------

# Encryption password (sensitive)
encryption.password = xxxx

# ----------------------------------------------------------------------
# File store [Optional]
# ----------------------------------------------------------------------

# File store provider. Currently 'filesystem' (default), 'aws-s3' and 's3' are supported.
filestore.provider = filesystem

# Directory / bucket name, folder below DHIS2_HOME on file system, 'bucket' in AWS S3
filestore.container = files

# URL where the S3 compatible API can be accessed (only for provider 's3')
filestore.endpoint = http://minio:9000 

# Datacenter location (not required)
filestore.location = eu-west-1

# Public identity / username
filestore.identity = dhis2-id

# Secret key / password (sensitive)
filestore.secret = xxxx

# ----------------------------------------------------------------------
# LDAP [Optional]
# ----------------------------------------------------------------------

# LDAP server URL
ldap.url = ldaps://300.20.300.20:636

# LDAP manager user distinguished name
ldap.manager.dn = cn=JohnDoe,ou=Country,ou=Admin,dc=hisp,dc=org

# LDAP manager user password (sensitive)
ldap.manager.password = xxxx

# LDAP entry distinguished name search base
ldap.search.base = dc=hisp,dc=org

# LDAP entry distinguished name filter
ldap.search.filter = (cn={0})

# ----------------------------------------------------------------------
# Node [Optional]
# ----------------------------------------------------------------------

# Node identifier, optional, useful in clusters
node.id = 'node-1'

# ----------------------------------------------------------------------
# Monitoring [Optional]
# ----------------------------------------------------------------------

# DHIS2 API monitoring
monitoring.api.enabled = on

# JVM monitoring
monitoring.jvm.enabled = on

# Database connection pool monitoring
monitoring.dbpool.enabled = on

# Hibernate monitoring, do not use in production
monitoring.hibernate.enabled = off

# Uptime monitoring
monitoring.uptime.enabled = on

# CPU monitoring
monitoring.cpu.enabled = on

# ----------------------------------------------------------------------
# Analytics [Optional]
# ----------------------------------------------------------------------

# Analytics server-side cache expiration in seconds
analytics.cache.expiration = 3600

# Analytics unlogged tables. Accepts on/off. It's `on` by default. If enabled, this will boost the analytics table export process significantly.
# But this comes with a cost: "unlogged" tables cannot be replicated. It means that clustering won't be possible. Also, analytics tables will be automatically truncated if PostgreSQL is suddenly reset (abrupt reset/crash). If PostgreSQL is reset gracefully, it won't impact any table. In this case, the analytics tables will remain in place accordingly. If you cannot afford the costs mentioned above, you should disable it (set to `off`).
analytics.table.unlogged = on

# ----------------------------------------------------------------------
# System telemetry [Optional]
# ----------------------------------------------------------------------

# System monitoring URL
system.monitoring.url = 

# System monitoring username
system.monitoring.username = 

# System monitoring password (sensitive)
system.monitoring.password = xxxx

# ----------------------------------------------------------------------
# System update notifications [Optional]
# ----------------------------------------------------------------------

system.update_notifications_enabled = on

# ----------------------------------------------------------------------
# App Hub [Optional]
# ----------------------------------------------------------------------

# Base URL to the DHIS2 App Hub service
apphub.base.url = https://apps.dhis2.org"
# Base API URL to the DHIS2 App Hub service, used for app updates
apphub.api.url = https://apps.dhis2.org/api


# Number of possible concurrent sessions on different computers or browsers for each user. If configured to 1, the
# user will be logged out from any other session when a new session is started.
max.sessions.per_user = 10
```

## 变更日志 { #install_changelog } 

当某些实体在系统中更改时，DHIS2将条目写入更改日志。实体分为两类：_Aggregate_和_tracker_。 _aggregate_类别包括对汇总数据值的更改。 _tracker_类别包括对项目实例，项目临时所有权项，跟踪的实体属性值和跟踪的实体数据值的更改。

The changelog for both categories are enabled by default. You can control whether to enable or disable the changelog by category through the `dhis.conf` configuration file using the properties described below. Property options are `on` (default) and `off`.

更改日志的好处是能够查看已对数据执行的更改。禁用更改日志的好处是，通过避免将更改日志项写入数据库的成本以及较少使用的数据库存储，可以对性能进行较小的改进。建议启用变更日志，如果禁用它，则应格外小心。

```属性
＃汇总变更日志，可以为“ on”，“ off”
changelog.aggregate =开启

＃Tracker changelog，可以为“ on”，“ off”
changelog.tracker =开
```

## 应用程序日志记录 { #install_application_logging } 

本节介绍DHIS 2中的应用程序日志记录。

### 日志文件 { #log-files } 

DHIS2应用程序日志输出定向到多个文件和位置。首先，将日志输出发送到标准输出。 Tomcat Servlet容器通常将标准输出输出到“ logs”下的文件：

     <tomcat-dir> /logs/catalina.out

Second, log output is written to a "logs" directory under the DHIS2 home directory as defined by the `DHIS2_HOME` environment variables. There is a main log file for all output, and separate log files for various
background processes. The main file includes the background process logs as well. The log files are capped at 50 Mb and log content is continuously appended.

     <DHIS2_HOME> /logs/dhis.log
     <DHIS2_HOME> /logs/dhis-analytics-table.log
     <DHIS2_HOME> /logs/dhis-data-exchange.log
     <DHIS2_HOME> /logs/dhis-data-sync.log

### 日志配置 { #log-configuration } 

To override the default log configuration you can specify a Java system
property with the name `log4j2.configurationFile` and a value pointing to the
[Log4j version 2](https://logging.apache.org/log4j/2.x/manual/configuration.html)
configuration file at the file system like this:

```properties
-Dlog4j2.configurationFile=/home/dhis/config/log4j2.properties
```

可以设置Java系统属性，例如通过* JAVA \ _OPTS *环境变量或tomcat启动脚本中。

A second approach to overriding the log configuration is to specify logging properties in the `dhis.conf` configuration file. The supported properties are:

```属性
＃日志文件的最大大小，默认为'100MB'
logging.file.max_size = 250MB

＃最大滚动日志归档文件数，默认为0
logging.file.max_archives = 2
```

DHIS2 will eventually phase out logging to standard out / catalina.out and as a result it is recommended to rely on the logs under `DHIS2_HOME`.

DHIS2 will provide the following context values:

* `sessionId`: Current user's session ID
* `xRequestID`: An alphanumeric ID as send by the `X-Request-ID` HTTP header for the currently processed request; empty if not provided

To use the context variables in the log add them using `-X{<name>}` to your log pattern as in this example:

    * %-5p %d{ISO8601} %m (%F [%t]) %X{sessionId} %X{xRequestID}%n

### Log level configuration { #log-level-configuration } 

To set the log level of individual packages you can specify properties on the format  `logging.level.{package-names}` in `dhis.conf`. For example, to set the the log level for the entire Spring Framework to DEBUG and up, you can specify:

```
logging.level.org.springframework = DEBUG
```
To set the log level to DEBUG for the DHIS2 services, you can specify:

```
logging.level.org.hisp.dhis = DEBUG
```

常见的日志级别是`DEBUG`、`INFO`、`WARN`和`ERROR`。

> **Note**
> 
> Log level configuration is not supported for the embedded DHIS2 Jetty version.

## 使用PostgreSQL数据库 { #install_working_with_the_postgresql_database } 

Common operations when managing a DHIS2 instance are dumping and restoring databases. Note that when making backups of the DHIS 2 database, it is good practise to exclude tables which are generated by the system, such as the resource and analytics tables. To make a dump (copy) of your database to a file,  you can invoke the following command.

```bash
pg_dump {database} -U {user} -T "_*" -T "analytics*"  -f {filename}
```
In the following example, the database name is `dhis2`, the user is `dhis` and the output filename is `dhis2.sql`:

```bash
pg_dump dhis2 -U dhis -T "analytics*" -T "_*" -f dhis2.sql
```

It is good practice to compress the If you want to compress the output file with `gzip`, which can be done like this:

```bash
pg_dump dhis2 -U dhis -T "analytics*" -T "_*" | gzip > dhis2.sql.gz
```

要在另一个系统上恢复数据库副本，您首先需要创建一个空数据库，如安装部分所述。如果您创建了压缩版本，还需要`gunzip`副本。要恢复副本，您可以调用以下命令：

```bash
psql -d dhis2 -U dhis -f dhis2.sql
```



# Upgrading { #upgrading-dhis2 }

## Upgrading vs. Updating { #upgrading-vs-updating }

When we talk about upgrading DHIS2, we generally simply mean "moving to a newer version". However, there is an important distinction between *upgrading* and *updating*.

**Upgrading**
:   Moving to a newer base version of DHIS2 (for example, from 2.34 to 2.36). Upgrading typically requires planning, testing, training (for new features or interfaces), which may take significant time and effort.

**Updating**
:   Moving to a newer patch of the current DHIS2 version (for example, from 2.35.1 to 2.35.4). Updating mainly provides bug fixes without changing the functionality of the software. It is lower risk, and we advise everyone to keep their version up to date.

## Before you begin { #upgrading-before-you-begin }

> **Caution**
>
> It is important to note that once you upgrade you will not be able to use the upgraded database with an older version of DHIS2. That is to say **it is not possible to downgrade**.
>
> If you wish to revert to an older version, you must do so with a copy of the database that was created from that older version, or a previous version. Therefore, it is almost always a good idea to make a copy of your database before you uprgrade.

## Performing the upgrade { #upgrading-process }

Regardless of whether you are upgrading or updating, the technical process is more-or-less identical. We will just refer to it as upgrading.

### 1 Safeguard your data { #upgrading-safeguard-your-data }

Depending on what sort of DHIS2 instance you have, and what you use it for, the first step is to make sure that you can recover any important data if anything goes wrong with the upgrade.

This means performing standard system admin tasks, such as:

1. Backing up your database
2. Testing in a development environment
3. Scheduling down time (to avoid data being entered during the upgrade)
4. etc.

### 2 Upgrade the software { #upgrading-upgrade-the-software }

#### From v2.29 or below { #upgrading-pre-230 }

If you are starting from v2.29 or below, you must first upgrade to v2.30 version-by-version, manually, following the upgrade notes you find under the specific version numbers on [our releases site](https://github.com/dhis2/dhis2-releases). When you are at v2.30 you can go to the next section.

#### From v2.30 or above { #upgrading-post-230 }

If you are starting from at least v2.30:

1. **Read all of the upgrade notes from your current version up to the target version on [our releases site](https://github.com/dhis2/dhis2-releases).** Make sure your environment meets all of the requirements
2. Stop the server
3. Make a final copy of your database (and ensure it is not corrupted)
4. Drop any materialized SQL views from your database
5. Replace the war file with the target version (There is no need to upgrade to intermediate versions; in fact, it is not recommended)
6. 启动服务器

You should now be ready to enjoy the new fixes and features.



# 监控 { #monitoring } 

## 介绍 { #monitoring } 

DHIS2可以导出[Prometheus]（https://prometheus.io/）兼容的度量标准，以监视DHIS2节点。

本节介绍了使用标准安装过程（`apt-get`）和Docker安装Prometheus和[Grafana]（https://grafana.com/）以及配置Grafana以显示DHIS2指标所需的步骤。

有关DHIS2实例公开的指标的列表，请参阅[GitHub]（https://github.com/dhis2/wow-backend/blob/master/guides/monitoring.md）上的监视指南。

## 设定 { #monitoring_setup } 

下一节将介绍如何设置Prometheus和Grafana，以及如何设置Prometheus从一个或多个DHIS2实例中提取数据。

### 在Ubuntu和Debian上安装Prometheus + Grafana { #prometheus } 

- 从官方[下载]（https://prometheus.io/download/）页面下载Prometheus。

- 确保针对您的操作系统和CPU架构（Linux和amd64）进行过滤。

- 确保选择最新的稳定版本，而不是“ rc”版本，因为目前尚不足够稳定。

- 通过单击链接或使用`wget`下载档案。

```
wget https://github.com/prometheus/prometheus/releases/download/v2.15.2/prometheus-2.15.2.linux-amd64.tar.gz
```

- 解开拉链

```
tar xvzf prometheus-2.15.2.linux-amd64.tar.gz
```

归档文件包含许多重要文件，但这是您需要了解的主要文件。

- `prometheus.yml`：Prometheus的配置文件。这是您将要修改的文件，以调整Prometheus服务器，例如，更改抓取间隔或配置自定义警报。
- `Prometheus`：Prometheus服务器的二进制文件。这是您要执行的命令，用于在Linux机器上启动Prometheus实例。
- `promtool`：这是一个可以运行以验证Prometheus配置的命令。

### 将Prometheus配置为服务 { #prometheus_service } 

- 使用`Prometheus`组创建一个`Prometheus`用户。

```
useradd -rs / bin / false普罗米修斯
```

- 将Prometheus二进制文件移到本地bin目录

```
cd prometheus-2.15.2.linux-amd64 /
cp prometheus promtool / usr / local / bin
chown prometheus：普罗米修斯/ usr / local / bin / prometheus
```

- 在Prometheus的`/ etc`文件夹中创建一个文件夹，并将控制台文件，控制台库和Prometheus配置文件移动到此新创建的文件夹中。

```
mkdir / etc / prometheus
cp -R控制台/ console_libraries / prometheus.yml / etc / prometheus
```

在根目录中创建一个数据文件夹，其中包含一个prometheus文件夹。

```
mkdir -p /data/prometheus
chown -R prometheus:prometheus /data/prometheus /etc/prometheus/*
```

### 创建Prometheus服务 { #prometheus_create_service } 

要创建Prometheus _systemd_服务，请转到`/ lib / systemd / system`文件夹并创建一个名为`prometheus.service`的新systemd文件。

```
cd / lib / systemd / system
touch prometheus.service
```

- 编辑新创建的文件，然后将以下内容粘贴到其中：

```属性
[单元]
描述=普罗米修斯
Wants = network-online.target
之后= network-online.target

[服务]
类型=简单
用户= prometheus
组=普罗米修斯
ExecStart = / usr / local / bin / prometheus \
  --config.file = / etc / prometheus / prometheus.yml \
  --storage.tsdb.path =“ / data / prometheus” \
  --web.console.templates = / etc / prometheus / consoles \
  --web.console.libraries = / etc / prometheus / console_libraries \
  --web.listen-address = 0.0.0.0：9090 \
  --web.enable-admin-api

重启=总是

[安装]
WantedBy =多用户目标
```

- 保存文件并在启动时启用Prometheus服务

```
systemctl启用普罗米修斯
systemctl启动方法
```

- 测试服务是否正在运行

```
系统状态法

...
活动：活动（运行中）
```

- 现在应该可以通过访问 `http://localhost:9090` 来访问Prometheus UI。 


### 设置Nginx反向代理 { #prometheus_nginx } 

Prometheus本身不支持身份验证或TLS加密。如果必须将Prometheus暴露在本地网络的边界之外，则启用身份验证和TLS加密非常重要。以下步骤显示了如何将Nginx用作反向代理。

- 安装Nginx（如果尚未安装）

```
apt更新
apt-get安装nginx
```

默认情况下，Nginx将开始在默认的`http`端口`80`中侦听HTTP请求。

如果机器上已经有一个Nginx实例在运行，并且您不确定它在哪个端口上侦听，请运行以下命令：

```
> lsof | grep LISTEN | grep nginx

nginx 15792根8u IPv4 1140223421 0t0 TCP *：http（LISTEN）
```

最后一列显示Nginx使用的端口（`http`->` 80`）。

默认情况下，Nginx配置位于`/ etc / nginx / nginx.conf`中。

确保` nginx.conf`包含`虚拟主机配置`部分

```
##
＃虚拟主机配置
##

包括/etc/nginx/conf.d/*.conf;
包括/ etc / nginx / sites-enabled / *;

```

- 在`/ etc / nginx / conf.d`中创建一个名为`prometheus.conf`的新文件

```
触摸/etc/nginx/conf.d/prometheus.conf
```

- 编辑新创建的文件，然后将以下内容粘贴到其中：

```
服务器{
  听1234;

  位置 / {
    proxy_pass http：// localhost：9090 /;
  }
}
```

- 重新启动Nginx并浏览到http：// localhost：1234

```
systemctl重启nginx

＃如果出现启动错误
journalctl -f -u nginx.service
```

- 通过编辑`/ lib / systemd / system / prometheus.service`，将Prometheus配置为反向代理，并将以下参数添加到传递给Prometheus可执行文件的参数列表中。

```
--web.external-url = https：// localhost：1234
```

- 重新启动服务

```
systemctl守护程序重新加载
systemctl重新启动prometheus


＃发生错误时
journalctl -f -u prometheus.service
```

### 启用反向代理身份验证 { #prometheus_auth } 

本节说明如何通过反向代理配置基本身份验证。如果您需要其他身份验证机制（SSO等），请检查相关文档。

- 确保系统上已安装` htpasswd`

```
apt-get install apache2-utils
```

- 创建认证文件

```
cd / etc / prometheus
htpasswd -c .credentials管理员
```

选择一个强密码，并确保正确创建了密码文件。

- Edit the previously created Nginx configuration file (`/etc/nginx/conf.d/prometheus.conf`), and add the authentication information.

```
服务器{
  听1234;

  位置 / {
    auth_basic“ Prometheus”;
    auth_basic_user_file /etc/prometheus/.credentials;
    proxy_pass http：// localhost：9090 /;
  }
}
```

- 重新启动Nginx

```
systemctl重启nginx

＃发生错误时
journalctl -f -u nginx.service
```

- `http://localhost:1234` 现在应该提示输入用户名和密码。

### 在Ubuntu和Debian上安装Grafana { #grafana } 

- 添加一个`gpg`密钥并从APT回购中安装OSS Grafana软件包

```sh
apt-get install -y apt-transport-https

wget -q -O-“ https://packages.grafana.com/gpg.key” | sudo apt键添加-

add-apt-repository“ deb https://packages.grafana.com/oss/deb稳定主程序”

apt-get更新

apt-get install grafana
```

- 如果系统使用`systemd`，则会自动创建一个新的`grafana-service`。检查`systemd`文件以获得有关Grafana安装的一些见解

```
猫/usr/lib/systemd/system/grafana-server.service
```

该文件非常重要，因为它提供有关新安装的Grafana实例的信息。

该文件显示：

The **Grafana server binary** is located at `/usr/sbin/grafana-server`.
The file that defines all the **environment variables** is located at `/etc/default/grafana-server`
The **configuration file** is given via the `CONF_FILE` environment variable.
The **PID of the file** is also determined by the `PID_FILE_DIR` environment variable.
**Logging**, **data**, **plugins** and **provisioning** paths are given by environment variables.

- 启动服务器

```
systemctl启动grafana服务器
```

- 访问Grafana Web控制台：http：// localhost：3000

The default login for Grafana is `admin` and the default password is also `admin`.
You will be prompted to change the password on first access.

- 将Prometheus配置为Grafana数据源

通过左侧菜单中的`配置`>`数据源`，访问数据源面板。

点击`添加数据源`

在下一个窗口中选择Prometheus数据源。

根据Prometheus设置配置数据源（使用身份验证，TSL等）

### 使用Docker安装Prometheus + Grafana { #prometheus_grafana_docker } 

本节介绍如何启动包含Prometheus和Grafana的Prometheus堆栈。

配置基于以下项目：https://github.com/vegasbrianc/prometheus

- 克隆这个Github项目：https://github.com/vegasbrianc/prometheus

- 使用以下命令启动Prometheus堆栈：

```
docker stack deploy -c docker-stack.yml舞会
```

上面的命令，可能会导致以下错误：

*此节点不是群集管理器。使用“ docker swarm init”或“ docker swarm join”将此节点连接到swarm并重试*

如果发生这种情况，则需要启动Swarm。您可以使用以下命令行：

```
docker swarm初始化--advertise-addr <YOUR_IP>
```

一旦该命令成功运行，您就应该能够运行上一个命令而不会出现问题。

该堆栈还包含用于Docker监视的Node导出器。如果您对Docker监控不感兴趣，可以在`docker-stack.yml`文件中注释掉相关部分：

- `node-exporter`
- `cadvisor`

- 要停止Prometheus堆栈：

```
docker stack rm舞会
```

Prometheus配置文件（` prometheus.yml`）位于`prometheus`文件夹中。

- 通过以下网址访问Grafana Web控制台：http：// localhost：3000，用户名：`admin`和密码：`foobar`

### 配置Prometheus从一个或多个DHIS2实例提取指标 { #prometheus_dhis2 } 

在使用Prometheus之前，需要进行基本配置。因此，我们需要创建一个名为`prometheus.yml`的配置文件。

> **注意**
>
> Prometheus的配置文件是用YAML编写的，严格禁止使用制表符。如果文件格式错误，Prometheus将不会启动。编辑时要小心。

Prometheus的配置文件分为三个部分：`global`，` rule_files`和`scrape_configs`。

在全局部分中，我们可以找到Prometheus的常规配置：`scrape_interval`定义Prometheus刮除目标的频率，`evaluation_interval`控制软件评估规则的频率。规则用于创建新的时间序列并用于生成警报。

`rule_files`块包含我们希望Prometheus服务器加载的任何规则的位置信息。

配置文件的最后一块名为` scape_configs`，其中包含Prometheus监视的资源信息。

一个简单的DHIS2 Prometheus监视文件如下例所示：

```yaml
global:
  scrape_interval:     15s
  evaluation_interval: 15s 

scrape_configs:
  - job_name: 'dhis2'
    metrics_path: '/api/metrics'
    basic_auth:
      username: admin
      password: district
    static_configs:
      - targets: ['localhost:80']
```

全局的`scrape_interval`设置为15秒，对于大多数用例来说已经足够了。

In the `scrape_configs` part we have defined the DHIS2 exporter.
The `basic_auth` blocks contains the credentials required to access the `metrics` API: consider creating an ad-hoc user only for accessing the `metrics` endpoint.

Prometheus可以与DHIS2在同一服务器上运行，也可以不在同一服务器上运行：在上述配置中，假定Prometheus仅监视一个与Prometheus在同一服务器上运行的DHIS2实例，因此我们使用`localhost`。

### 配置DHIS2导出器 { #dhis2_metrics_conf } 

DHIS2中默认情况下禁用监视子系统。

必须明确启用每个指标群集，以便导出指标。要将DHIS2配置为导出一个或多个指标，请检查此[document]（https://github.com/dhis2/wow-backend/blob/master/guides/monitoring.md#dhis2-monitoring-configuration）。




# 审核 { #audit } 

## 介绍 { #introduction } 

DHIS2 supports a new audit service based on _Apache ActiveMQ Artemis_. Artemis is used as an asynchronous messaging system by DHIS2.

After an entity is saved to database, an audit message will be created and sent to the Artemis message consumer service. The message will then be processed in a different thread.

Audit logs can be retrieved from the DHIS2 database. Currently there is no UI or API endpoint available for retrieving audit entries.

Detailed explanation of the audit system architecture can be found [here](https://github.com/dhis2/wow-backend/blob/master/guides/auditing.md).

## What we log { #what_we_log }

This is the list of operations we log as part of the audit system:

- Operations on user accounts (like but not limited to creation, profile edits)
- Operations on user roles, groups and authority groups
- Operations on metadata objects (like but not limited to categories, organization units, reports)
- Operations on tracked objects (like but not limited to instances, attributes, data values)
- Jobs configuration
- Breaking the glass operations

## 单审核表 { #audit_table } 

All audit entries, except the ones related to tracked entities, will be saved into one single table named `audit`

| 柱     | 类型                        | 描述 |
|------------|-----------------------------|---------------------------------------------------------------------------------------------------------------------------------------------|
| 受审核者    | 整数                     | Primary key. |
| 审核类型  | 文本                        | 读取，创建，更新，删除，搜索 |
| 审计范围 | 文本                        | 元数据，汇总，跟踪器 |
| 克拉斯      | 文本                        | Audit Entity Java class name. |
| 属性 | jsonb                       | A JSON string with attributes of the audited object. Example: `{"valueType":"TEXT", "categoryCombo":"SWQW313FQY", "domainType":"TRACKER"}`. |
| 数据       | 比蒂亚                       | Compressed JSON string of the audit entity in byte array format (not humanly readable). |
| 创建于  | 没有时区的时间戳 | Time of creation. |
| 由...制作  | 文本                        | Username of the user performing the audited operation. |
| uid        | 文本                        | The UID of the audited object. |
| 码       | 文本                        | The code of the audited object. |

The audit service makes use of two new concepts: *Audit Scope* and *Audit Type*.

## 审核范围 { #audit_scope } 

An audit scope is a logical area of the application which can be audited. Currently there are three audit scopes.

| **Scope** | 键       | Audited objects                                              |
| --------- | --------- | ------------------------------------------------------------ |
| 追踪器   | tracker   | Tracked Entity Instance, Tracked Entity Attribute Value, Enrollment, Event. |
| 元数据  | metadata  | All metadata objects (e.g. Data Element, Organisation Unit). |
| Aggregate | aggregate | Aggregate Data Value.                                        |


## 审核类型 { #audit_type } 

An audit type is an action that triggers an audit operation. Currently we support the following four types.

| 名称     | 键      | 描述         |
| -------- | -------- | ------------------- |
| Read     | READ     | Object was read.    |
| Create   | CREATE   | Object was created. |
| Update   | UPDATE   | Object was updated. |
| 删除   | 删除   | Object was deleted. |
| Disabled | DISABLED | Disable audit.      |

> **Caution**
>
> The READ audit type may generate a lot of data in the database and may have an impact on the performance.


## Tracked entity audits { #tracked-entity-audits } 

Operations on tracked entities like instances, attributes and values are stored, respectively in the `trackedentityinstanceaudit`, `trackedentityattributevalueaudit` and `trackedentitydatavalueaudit` tables.

### trackedentityinstanceaudit { #trackedentityinstanceaudit } 

| 柱     | 类型                        | 描述 |
|------------|-----------------------------|-------------|
| trackedentityinstanceauditid | 整数 | Primary key. |
| trackedentityinstance | 文本  | Tracked entity instance name.  |
| created  | 没有时区的时间戳 | Time of creation. |
| accessedby | 文本 | Username of the user performing the audited operation. |
| 审核类型 | 文本 | 读取，创建，更新，删除，搜索 |
| comment | 文本 | The code of the audited object. |

This data can be retrieved via [API](#webapi_tracked_entity_instance_audits).

### trackedentityattributevalueaudit { #trackedentityattributevalueaudit } 

| 柱     | 类型                        | 描述 |
|------------|-----------------------------|-------------|
| trackedentityattributevalueauditid | 整数 | Primary key. |
| trackedentityinstanceid | 整数 | Instance ID of which the attribute value belongs to.  |
| trackedentityattributeid | 整数 | Attribute ID.  |
| created  | 没有时区的时间戳 | Time of creation. |
| modifiedby  | 文本 | Username of the user performing the audited operation. |
| 审核类型 | 文本  | 读取，创建，更新，删除，搜索 |
| 价值 | 文本 | The value of the audited object. |
| encryptedvalue | 文本 | The encrypted value if confidentiality flag is set. |


This data can be retrieved via [API](#webapi_tracked_entity_attribute_value_audits).

### trackedentitydatavalueaudit { #trackedentitydatavalueaudit } 

| 柱     | 类型                        | 描述 |
|------------|-----------------------------|-------------|
| trackedentitydatavalueauditid | 整数 | Primary key. |
| programstageinstanceid | 整数 | Program stage ID of which the data value belongs to.  |
| dataelementid | 整数 | ID of the data element.  |
| created | 没有时区的时间戳 | Time of creation. |
| modifiedby | 文本 | Username of the user performing the audited operation. |
| 审核类型 | 文本 | 读取，创建，更新，删除，搜索 |
| 价值 | 文本 | The value of the audited object. |
| providedelsewhere | bool | Indicates whether the user provided the value elsewhere or not. |

This data can be retrieved via [API](#webapi_tracked_entity_data_value_audits).

## 打破玻璃 { #breaking-the-glass } 
Breaking the glass features allows to access records a DHIS2 user doesn't have access in special circumstances. As a result of such, users must enter a reason to access such records.

A video explaining how it works can be found in our Youtube channel [here](https://www.youtube.com/watch?v=rTwg5Ix_E_M).

The breaking the glass event is stored in the `programtempownershipaudit` table, described below:

| 柱     | 类型  | 描述 |
|------------|-------|-------------|
| programtempownershipauditid | 整数 | Primary key. |
| programid | 整数 | Program ID of which the tracked entity belongs to.  |
| trackedentityinstanceid | 整数 | Instance ID of which the attribute value belongs to.  |
| created  | 没有时区的时间戳 | Time of creation. |
| accessedby  | 文本 | Username of the user performing the audited operation. |
| reason       | 文本 | The reason as inserted in the dialog. |


## 设定 { #audit_configuration } 

The audit system is enabled by default for the following scopes and types.

Scopes (case sensitive):

- `CREATE`
- `UPDATE`
- `DELETE`

Types:

- `METADATA`
- `TRACKER`
- `AGGREGATE`

This means that **no action is required** to enable the default audit system. The default setting is equivalent to the following `dhis.conf` configuration.

```properties
audit.metadata = CREATE;UPDATE;DELETE
audit.tracker = CREATE;UPDATE;DELETE
audit.aggregate = CREATE;UPDATE;DELETE
```

可以使用_审计矩阵_配置审计。审计矩阵表示范围和类型的有效组合，并在` dhis.conf` 配置文件中使用以下属性定义。每个属性都接受以分号 (`;`) 分隔的审计类型列表。

* `audit.metadata`
* `audit.tracker`
* `audit.aggregate`

### Artemis { #artemis } 
[Apache ActiveMQ Artemis](https://activemq.apache.org/components/artemis/documentation/) is an open source project to build a multi-protocol, embeddable, very high performance, clustered, asynchronous messaging system. It has been part of DHIS2 since version 2.31 and used as a system to consume audit logs.

By default, DHIS2 will start an embedded Artemis server, which is used internally by the application to store and access audit events.

However, if you have already an Artemis server, you can connect to it from DHIS2 to send audit events, as described in our [official documentation](#webapi_amqp_configuration): in this setup, audit events will flow from DHIS2 to the external Artemis system.

### log4j2 { #log4j2 } 
[log4j2](https://logging.apache.org/log4j/2.x/index.html) is the default DHIS2 logging library used to handle output messages. It's used to control what events are recored in which file.

The application ships a [log4j2 default configuration file](https://github.com/dhis2/dhis2-core/blob/master/dhis-2/dhis-web/dhis-web-commons-resources/src/main/webapp/WEB-INF/classes/log4j2.xml), which instructs what information to log and where (console). DHIS2 then takes care of import that file and instruction logging as described in the [log4j configuration class](https://github.com/dhis2/dhis2-core/blob/2.38/dhis-2/dhis-support/dhis-support-system/src/main/java/org/hisp/dhis/system/log/Log4JLogConfigInitializer.java), that is, redirecting output from console to files.

From 2.36 to 2.38, audit log file `dhis-audit.log` is rotated [every day at midnight](https://github.com/dhis2/dhis2-core/blob/2.38/dhis-2/dhis-support/dhis-support-system/src/main/java/org/hisp/dhis/system/log/Log4JLogConfigInitializer.java#L171).

An example of custom log4j2 configuration can be found [here](): it shows how to configure DHIS2 to save all logs into an external storage location, rotate them on a weekly basis and retain them for 30 days. Please read the [application logging section](#install_application_logging) on how to use it.

## 例子 { #examples } 

This section demonstrates how to configure the audit system in `dhis.conf`.

To enable audit of create and update of metadata and tracker only:

```properties
audit.metadata = CREATE;UPDATE
audit.tracker = CREATE;UPDATE
audit.aggregate = DISABLED
```

To only audit tracker related objects create and delete:

```properties
audit.metadata = DISABLED
audit.tracker = CREATE;DELETE
audit.aggregate = DISABLED
```

To completely disable audit for all scopes:
```properties
audit.metadata = DISABLED
audit.tracker = DISABLED
audit.aggregate = DISABLED
```

We recommend keeping the audit trails into a file, as by default in version 2.38. For older versions, the following configuration saves the audit logs into the `$DHIS2_HOME/logs/dhis-audit.log` file:
```properties
audit.database = off
audit.logger = on
```

To store audit data into the database, add the following to your `dhis.conf` file (default up until version 2.38):
```properties
audit.database = on
audit.logger = off
```

To extract logs from the `audit` table, you can use [`dhis2-audit-data-extractor`](https://github.com/dhis2/dhis2-utils/tree/master/tools/dhis2-audit-data-extractor) from the system where DHIS2 is running:
```
$ python extract_audit.py extract
```

Please read the documentation for full details.

To parse entries from log file, you can use the python script as follow:
```
$ grep "auditType" dhis-audit.log | python extract_audit.py parse
```

Or use `jq` as follow:

```
$ grep "auditType" dhis-audit.log | jq -r .
```

To select events within a specific date, you can use `jq` as follow (in this example, we're selecting all events happened between January 2022 and end of June 2022):

```
$ grep "auditType" dhis-audit.log | jq -r '.[] | select ( (.datetime >="2022-01-01") and (.datetime <= "2022-06-30") )'
```

Same with `extract_audit`:
```
$ python3 extract_audit.py extract -m stdout -f JSON | jq -r '.[] | select ( (.datetime >="2022-01-01") and (.datetime <= "2022-06-30") )'
```


# 使用网关进行SMS报告  { #sms_report_sending } 

DHIS2 supports accepting data via [SMS](https://docs.dhis2.org/master/en/dhis2_user_manual_en/mobile.html), however, the SMS needs to be compressed. The DHIS2 Android App acts as a transparent layer to send the information via SMS where the user does not have to worry about writing the SMS. To send SMSs with the Android App the SMS gateway need to be properly configured. This section explains the different options available and how to achieve that.

## 发送短信 { #sms_report_sening } 

It is important to clarify firstly, that this section mainly concerns the set up of **receiving SMS** (from mobile devices to the DHIS2 server), which is necessary when considering using the App to send (sync) information recorded in the app to the DHIS2 server via SMS. In the App this can be set-up under the *Settings* > *SMS Settings*

将SMS（即从DHIS2服务器发送到移动设备）的设置相对简单。如果仅需要在发生某些事件（消息，阈值等）时从DHIS2向用户电话发送通知，则仅需要发送SMS。

所有这些都可以在[移动配置部分]（https://docs.dhis2.org/master/en/user/html/mobile_sms_service.html）的SMS服务配置页面中进行配置。

常见的提供商（例如*批量SMS *和* Clickatell *）具有开箱即用的支持，并且两个提供商都支持将SMS发送到大多数国家/地区的号码。

另请注意，可以使用其他SMS网关来发送和接收SMS。因此，即使您在下面设置了用于接收SMS的解决方案，仍然可以使用上述上述解决方案之一来发送SMS。

## 使用Android设备作为SMS网关 { #sms_report_android_gateway } 

到目前为止，最简单的解决方案是使用专用的Android设备作为SMS网关。任何运行Android OS（4.4，Kitkat或更高版本）的手机或平板电脑都可以。为了将消息转发到您的DHIS2服务器，它将需要持续的Internet连接，并且还需要SIM卡来接收传入的SMS。

您需要在移动设备上下载并安装DHIS2 Android SMS网关应用程序。请参阅[版本]（https://github.com/dhis2/dhis2-sms-android-gateway/releases）列表，您可以在其中下载最新的APK文件进行安装。应用页面本身上有说明，但实际上，您只需要启动应用并输入DHIS2服务器的详细信息（URL，用户名和密码）即可。

设置并运行该网关后，您可以使用DHIS2 Capture App在任何其他移动设备的配置页面中输入此网关设备的电话号码。然后，当从这些报告设备发送SMS时，它们将在网关设备上接收并自动转发到DHIS2服务器，在该服务器上对其进行处理。

Using this gateway device is perfect for testing the SMS functionality but should not be used in production as it presents several flaws like not being able to handle multipart SMS, handling concurrent SMS and might even be killed by the Android OS.  Therefor when considering moving a project to production it would be necessary to investigate one of the more permanent and reliable solutions for gateways below.

### 使用Android设备网关发送短信 { #sending-sms-using-an-android-device-gateway } 

当前不支持也不记录此选项。

## 专用短信网关 { #sms_report_dedicated_gateway } 

本节讨论更永久和专用的SMS网关的使用以及可用的选项。下面的每个选项都将涉及一个提供商（或您自己）与国家/地区的电话运营商建立SMPP连接，并使用此连接来接收传入的SMS并使用HTTP通过Internet将其转发到您的DHIS2服务器。

这些解决方案可以使用**长号**或**短代码**。长号是大多数人使用的标准移动电话号码，即+61400123123。短代码只是短号，例如311。短代码通常会花费更多的时间来设置和维护。

### 确保输入到DHIS2服务器的SMS格式正确 { #ensuring-incoming-sms-to-dhis2-server-are-formatted-correctly } 

通过API将传入的SMS发送到DHIS2服务器时，请使用以下URL：* https：// <DHIS2_server_url> / api / sms / inbound *

在DHIS2版本2.34及更低版本中，此终结点要求入站SMS的格式必须具有非常特定的格式，即消息本身必须是一个名为text的参数，发件人的电话号码必须是一个名为originator的参数。

使用以下所有SMS网关选项时，将它们配置为将传入的SMS转发到另一个Web服务时，它们将各自具有自己的格式，该格式与DHIS2 API期望的格式不同。因此，有必要重新格式化它们，然后再将它们发送到DHIS2服务器。

一种选择是运行您自己的非常简单的Web服务，该服务仅接收来自网关提供商的传入SMS，将其重新格式化为DHIS2所需的格式，然后将其转发到DHIS2 API。此类服务需要由软件开发人员编写。

在DHIS2版本2.35中，计划使用传入SMS的模板系统来支持这些情况，因此您可以指定将从提供商处发送的消息的格式。这样，您可以将DHIS2服务器配置为接受来自任何其他SMS网关提供者的传入SMS，并且它们可以直接将传入SMS发送到DHIS2 API，而无需这种格式的Web服务。

### 使用RapidPro { #using-rapidpro } 

[RapidPro]（https://rapidpro.io/）是联合国儿童基金会在全球50多个国家/地区提供的服务。它是一套软件，可以与国内电话运营商合作，使组织能够为其项目设计SMS解决方案，例如SMS报告或宣传活动。

RapidPro服务将包括通常通过短码与国内一个或多个电话运营商的SMPP连接，这可能专用于NGO的卫生工作。然后可以添加一个Webhook，以便将传入的SMS转发到另一个Web服务，例如上述的格式化Web服务。如果该短代码也用于其他目的，则可能有必要将报告设备的电话号码添加到单独的组中，以便仅将来自那些设备的传入SMS转发到Webhook。

RapidPro目前在大约一半正在使用或试用DHIS2的国家/地区中建立和运行。在考虑下面的一种解决方案（可能在财务和时间上都可能会付出高昂的代价）之前，值得与Unicef联系，以确定RapidPro是否可用以及在您所在的国家/地区是否可以用于健康报告。

### 使用商业短信网关提供商 { #using-commercial-sms-gateway-providers } 

在上面的“发送SMS”部分提到的商业SMS网关提供商中，它们通常具有在大多数国家（地区）*发送* SMS的功能，但只能在少数国家/地区支持*接收* SMS。他们支持接收SMS的大多数国家/地区不是使用DHIS2的国家/地区。在使用DHIS2的国家/地区中，大多数已经在国内运行RapidPro服务了。

但是，值得研究您所在国家/地区可以使用哪些商业选项。在某些国家/地区，会有一些小型国家公司提供SMS服务，它们将与您可以使用的电话提供商建立现有的SMPP连接。

### 直接使用电话运营商 { #using-phone-carriers-directly } 

如果以上解决方案均不可用，则有必要直接与您所在国家的电话运营商联系。向他们询问的第一个问题是，他们是否知道与您有联系的与他们进行SMPP连接的任何公司。

如果不是这样，作为最后的选择，您将需要考虑与电话提供商建立并维护自己的SMPP连接。但是，并非所有电话提供商都可以提供这种服务。

您需要运行自己的服务器，该服务器上运行的软件例如[Kannel]（https://www.kannel.org/），该软件（通常通过VPN）连接到在电话提供商网络中运行的SMPP服务。有了此设置，任何配置的长号或短码的传入SMS都会从电话运营商发送到Kannel服务器，然后您可以按照上述方式转发这些消息。

### 接收串联或多部分短信 { #receiving-concatenated-or-multipart-sms } 

When syncing data via SMS with the DHIS2 Android App, it uses a compressed format to use as little space (characters of text) as possible. Despite this, it will quite often be the case that a message will extend over the 160 character limit of one standard SMS. On most modern mobile devices these messages will still be sent as one concatenated or multipart SMS, and received as one message. 

然后，在选择SMS网关时，重要的是确认所使用的电话运营商支持级联SMS。他们中的大多数人都将支持此功能，但是重要的是要进行确认，因为如果拆分SMS，SMS功能将无法使用。这依赖于称为UDH（用户数据头）的东西。与提供商讨论时，请确保您询问是否支持。



# Using the User Impersonation Feature in DHIS2 { #user_impersonation }

## 总览 { #overview } 

User impersonation, also known as user switching, is a powerful feature provided in DHIS2 for administrative users to
log in as another user. This feature is especially useful for troubleshooting or resolving user-related issues, as it
allows an administrator to experience DHIS2 exactly as the user does.

此功能基于 Spring Security 的`SwitchUserFilter`构建，但具有附加配置选项。

> **注意**
>
> 该功能默认**禁用**。要启用它，您必须设置 `switch_user_feature.enabled` 属性
> 为`true`
> 你的 `dhis.conf` 文件。
>
> 此功能被视为**实验性**，仅适用于从配置的 IP 地址调用。因此，为了
> 使用它
> 您必须知道要调用它的 IP 地址并配置 `switch_user_allow_listed_ips`
> 财产
> 在 `dhis.conf` 文件中。此限制将来可能会被取消。

## How It Works { #how-it-works } 

The user impersonation feature operates in the following manner:

1. 管理用户使用`username`向特定 URL（例如`/impersonate?username=USERNAME`）发出请求
   参数
   indicating the username of the user they wish to impersonate.

2. 用户冒充功能拦截此请求，将`SecurityContext`切换到新用户，并重定向。
   to the home page.

3. While impersonating another user, the administrative user can make requests as if they were the impersonated user.

4. To switch back to the original user, the administrative user makes a request to another URL (
   例如，`/impersonateExit`。用户冒充功能拦截此请求，切换`SecurityContext`。
   back to the original user, and redirects to the home page.

## How To Use { #how-to-use } 

Follow these steps to use the user impersonation feature:

1. 作为具有`ALL`或`F_IMPERSONATE_USER`权限的管理员用户登录。
2. 导航到用户冒充的URL（例如，`/impersonate?username=USERNAME`）。
3. 提供您希望冒充的用户的`用户名`参数。
4. The system will switch your session to that of the impersonated user, and you will be redirected to the home page.
5. Perform any actions necessary for troubleshooting or user support.
6. 当你完成后，请导航到URL以结束冒充（例如，`/impersonateExit`）。你的会话将被结束。
   switched back to your original administrative user.

## 组态 { #configuration } 

The user impersonation feature configuration options.

* `switch_user_feature.enabled` (Enable or disable the feature, default: `disabled`)
* `switch_user_allow_listed_ips` (Default allowed IP(s) are; `localhost,127.0.0.1,[0:0:0:0:0:0:0:1]`)

## Security restrictions { #security-restrictions } 
* Feature must be enabled in the `dhis.conf` configuration file, default value is; `disabled`.
* Users trying to impersonate need to send requests from an allowed IP.
* Users without the `ALL` authority can not impersonate another user that has the `ALL` authority.
* Users can not impersonate themselves.

## Security Implications { #security-implications } 

This feature should be used with caution due to its inherent security implications. Only trusted administrators should
be granted the capability to impersonate users. It's also recommended to pay attention to the log events related to the
user impersonation.

User impersonation events are logged in the following
format: `Authentication event: AuthenticationSwitchUserEvent; username: USER_DOING_THE_IMPERSONATION; targetUser: USER_BEING_IMPERSONATED;`

